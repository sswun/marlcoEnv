{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM (Collaborative Moving) Environment Tutorial\n",
    "\n",
    "This notebook provides a comprehensive tutorial for the Collaborative Moving (CM) environment, where multiple agents must work together to push a box to a target location.\n",
    "\n",
    "## Table of Contents\n",
    "1. Environment Overview\n",
    "2. Task Description\n",
    "3. Action Space\n",
    "4. Observation Space\n",
    "5. CTDE Global State Space\n",
    "6. Reward System\n",
    "7. Usage Examples\n",
    "8. Integration with MARL Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Overview\n",
    "\n",
    "The CM environment is a multi-agent collaborative task where agents must cooperate to push a box from its initial position to a goal location. This environment is designed to test cooperation, coordination, and planning capabilities of multi-agent reinforcement learning algorithms.\n",
    "\n",
    "**Key Features:**\n",
    "- 2-4 agents can collaborate\n",
    "- Box pushing requires cooperation (higher success probability with more agents)\n",
    "- Configurable grid sizes and difficulty levels\n",
    "- Support for both single-agent and multi-agent scenarios\n",
    "- Rich observation space including agent positions and relative information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Env.CM.env_cm import create_cm_env\n",
    "from Env.CM.env_cm_ctde import create_cm_ctde_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task Description\n",
    "\n",
    "### Objective\n",
    "Agents must cooperate to move a box from its starting position to a designated goal area.\n",
    "\n",
    "### Cooperation Mechanism\n",
    "- The box can only be moved successfully when agents push from different sides\n",
    "- Success probability increases with the number of cooperating agents:\n",
    "  - 1 agent: 50% success rate\n",
    "  - 2 agents: 75% success rate\n",
    "  - 3 agents: 90% success rate\n",
    "  - 4 agents: 100% success rate\n",
    "\n",
    "### Episode Termination\n",
    "- Box reaches the goal area\n",
    "- Maximum number of steps is exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Information:\n",
      "  n_agents: 2\n",
      "  agent_ids: ['agent_0', 'agent_1']\n",
      "  n_actions: 5\n",
      "  obs_dims: {'agent_0': 8, 'agent_1': 8}\n",
      "  act_dims: {'agent_0': 5, 'agent_1': 5}\n",
      "  episode_limit: 100\n",
      "  grid_size: 7\n",
      "  box_size: 2\n",
      "\n",
      "Difficulty Configurations:\n",
      "  Available difficulties: ['debug', 'easy', 'normal', 'hard']\n",
      "  CTDE difficulties: ['debug_ctde', 'easy_ctde', 'normal_ctde', 'hard_ctde']\n"
     ]
    }
   ],
   "source": [
    "# Create environment and demonstrate basic functionality\n",
    "env = create_cm_env(difficulty=\"normal\", render_mode=\"\")\n",
    "\n",
    "print(\"Environment Information:\")\n",
    "env_info = env.get_env_info()\n",
    "for key, value in env_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nDifficulty Configurations:\")\n",
    "print(\"  Available difficulties:\", ['debug', 'easy', 'normal', 'hard'])\n",
    "print(\"  CTDE difficulties:\", ['debug_ctde', 'easy_ctde', 'normal_ctde', 'hard_ctde'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Action Space\n",
    "\n",
    "Each agent has 5 discrete actions:\n",
    "\n",
    "| Action ID | Action Name | Description |\n",
    "|-----------|-------------|-------------|\n",
    "| 0 | STAY | Agent remains in current position |\n",
    "| 1 | UP | Agent moves one grid cell up |\n",
    "| 2 | DOWN | Agent moves one grid cell down |\n",
    "| 3 | LEFT | Agent moves one grid cell left |\n",
    "| 4 | RIGHT | Agent moves one grid cell right |\n",
    "\n",
    "### Action Constraints\n",
    "- Agents cannot move outside the grid boundaries\n",
    "- Agents cannot move into the box (they push it instead)\n",
    "- Multiple agents cannot occupy the same position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:\n",
      "  Number of actions: 5\n",
      "  Action space: Discrete(5)\n",
      "  agent_0 available actions: [0, 1, 2, 3, 4]\n",
      "  agent_1 available actions: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate action space\n",
    "env.reset()\n",
    "\n",
    "print(\"Action Space:\")\n",
    "print(f\"  Number of actions: {env.n_actions}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "\n",
    "# Show available actions for each agent\n",
    "for agent_id in env.agent_ids:\n",
    "    avail_actions = env.get_avail_actions(agent_id)\n",
    "    print(f\"  {agent_id} available actions: {avail_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Observation Space\n",
    "\n",
    "Each agent receives a local observation containing:\n",
    "\n",
    "**Vector format (length = 6 + 2*(n_agents-1)):**\n",
    "- `self_x, self_y`: Agent's own position (2 values)\n",
    "- `box_center_x, box_center_y`: Box center position (2 values)\n",
    "- `goal_center_x, goal_center_y`: Goal center position (2 values)\n",
    "- `relative_positions`: Relative positions of other agents (2*(n_agents-1) values)\n",
    "\n",
    "For 2-agent environment, observation length = 8:\n",
    "- [self_x, self_y, box_x, box_y, goal_x, goal_y, other_rel_x, other_rel_y]\n",
    "\n",
    "**Normalization:**\n",
    "- Observations are normalized by grid size when `normalize_observations=True`\n",
    "- Values range from 0.0 to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space:\n",
      "  Observation space: Box(0.0, 7.0, (8,), float32)\n",
      "  Observation dimension: 8\n",
      "\n",
      "Initial observations:\n",
      "  agent_0: [ 0.5714286   0.2857143   0.35714287  0.35714287  0.21428572  0.64285713\n",
      " -0.2857143  -0.14285715]\n",
      "    Shape: (8,)\n",
      "    Min: -0.286, Max: 0.643\n",
      "  agent_1: [0.2857143  0.14285715 0.35714287 0.35714287 0.21428572 0.64285713\n",
      " 0.2857143  0.14285715]\n",
      "    Shape: (8,)\n",
      "    Min: 0.143, Max: 0.643\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate observation space\n",
    "obs = env.reset()\n",
    "\n",
    "print(\"Observation Space:\")\n",
    "print(f\"  Observation space: {env.observation_space}\")\n",
    "print(f\"  Observation dimension: {env.observation_space.shape[0]}\")\n",
    "\n",
    "print(\"\\nInitial observations:\")\n",
    "for agent_id, observation in obs.items():\n",
    "    print(f\"  {agent_id}: {observation}\")\n",
    "    print(f\"    Shape: {observation.shape}\")\n",
    "    print(f\"    Min: {observation.min():.3f}, Max: {observation.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CTDE Global State Space\n",
    "\n",
    "For Centralized Training with Decentralized Execution (CTDE), the environment provides a global state that includes information from all agents.\n",
    "\n",
    "**Global State Components:**\n",
    "- All agent positions (2 * n_agents values)\n",
    "- Box position and size (3 values: x, y, size)\n",
    "- Goal position and size (3 values: x, y, size)\n",
    "- Relative positions between agents (2 * n_agents * (n_agents-1) values)\n",
    "\n",
    "**Global State Types:**\n",
    "- `concat`: Concatenation of all information (default)\n",
    "- `mean`: Mean pooling of agent observations\n",
    "- `max`: Max pooling of agent observations\n",
    "- `attention`: Attention-based aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTDE Environment:\n",
      "  Global state dimension: 16\n",
      "  Global state sample: [ 0.2857143   0.14285715  0.35714287  0.35714287  0.07142857  0.21428572\n",
      " -0.14285715  0.42857143  0.14285715  0.5714286 ]...\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate CTDE environment\n",
    "ctde_env = create_cm_ctde_env(difficulty=\"normal_ctde\", global_state_type=\"concat\")\n",
    "\n",
    "obs = ctde_env.reset()\n",
    "global_state = ctde_env.get_global_state()\n",
    "\n",
    "print(\"CTDE Environment:\")\n",
    "print(f\"  Global state dimension: {len(global_state)}\")\n",
    "print(f\"  Global state sample: {global_state[:10]}...\")  # Show first 10 values\n",
    "\n",
    "ctde_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reward System\n",
    "\n",
    "The reward system is designed to encourage cooperation and goal completion while discouraging random exploration.\n",
    "\n",
    "**Reward Components:**\n",
    "1. **Time Penalty**: -0.3 per step (encourages efficiency)\n",
    "2. **Distance Improvement**: 0.3 × distance reduction (only for significant progress > 0.2)\n",
    "3. **Box Movement**: 1.0 (only when box moves toward goal)\n",
    "4. **Cooperation Bonus**: 1.5 × (n_pushing_agents - 1)\n",
    "5. **Goal Completion**: 50.0 + efficiency bonus (up to +15.0)\n",
    "\n",
    "**Reward Span**: ~80 units (from random exploration to goal completion)\n",
    "\n",
    "**Key Design Principles:**\n",
    "- Random exploration yields low positive rewards (~5-10)\n",
    "- Meaningful progress provides moderate rewards (10-30)\n",
    "- Goal completion provides the highest reward (50-65)\n",
    "- Cooperation is explicitly rewarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward System Configuration:\n",
      "  Time penalty: -0.3\n",
      "  Goal reward: 50.0\n",
      "  Cooperation reward: 1.5\n",
      "  Box move reward: 1.0\n",
      "  Distance reward scale: 0.3\n",
      "\n",
      "Sample Episode Rewards:\n",
      "  Episode 1: Reward=-8.3, Steps=50, Goal Reached=False\n",
      "  Episode 2: Reward=66.1, Steps=18, Goal Reached=True\n",
      "  Episode 3: Reward=-5.5, Steps=50, Goal Reached=False\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate reward system\n",
    "print(\"Reward System Configuration:\")\n",
    "print(f\"  Time penalty: {env.config.time_penalty}\")\n",
    "print(f\"  Goal reward: {env.config.goal_reached_reward}\")\n",
    "print(f\"  Cooperation reward: {env.config.cooperation_reward}\")\n",
    "print(f\"  Box move reward: {env.config.box_move_reward_scale}\")\n",
    "print(f\"  Distance reward scale: {env.config.distance_reward_scale}\")\n",
    "\n",
    "# Run a few episodes to show reward distribution\n",
    "print(\"\\nSample Episode Rewards:\")\n",
    "for episode in range(3):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    for step in range(50):\n",
    "        actions = {}\n",
    "        for agent_id in env.agent_ids:\n",
    "            avail_actions = env.get_avail_actions(agent_id)\n",
    "            actions[agent_id] = np.random.choice(avail_actions)\n",
    "        \n",
    "        obs, rewards, dones, info = env.step(actions)\n",
    "        step_reward = list(rewards.values())[0]\n",
    "        total_reward += step_reward\n",
    "        step_count += 1\n",
    "        \n",
    "        if any(dones.values()):\n",
    "            print(f\"  Episode {episode+1}: Reward={total_reward:.1f}, Steps={step_count}, Goal Reached=True\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"  Episode {episode+1}: Reward={total_reward:.1f}, Steps={step_count}, Goal Reached=False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Usage Examples\n",
    "\n",
    "### Basic Environment Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Environment Usage:\n",
      "Episode completed in 100 steps!\n",
      "Total reward: -19.19\n",
      "Goal reached: False\n"
     ]
    }
   ],
   "source": [
    "# Basic usage example\n",
    "def run_basic_episode(env, max_steps=100):\n",
    "    \"\"\"Run a single episode with random actions.\"\"\"\n",
    "    obs = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Get random actions for all agents\n",
    "        actions = {}\n",
    "        for agent_id in env.agent_ids:\n",
    "            avail_actions = env.get_avail_actions(agent_id)\n",
    "            actions[agent_id] = np.random.choice(avail_actions)\n",
    "        \n",
    "        # Execute step\n",
    "        obs, rewards, dones, info = env.step(actions)\n",
    "        \n",
    "        # Accumulate reward (all agents get same reward)\n",
    "        step_reward = list(rewards.values())[0]\n",
    "        episode_reward += step_reward\n",
    "        \n",
    "        # Check if episode is done\n",
    "        if any(dones.values()):\n",
    "            print(f\"Episode completed in {step+1} steps!\")\n",
    "            print(f\"Total reward: {episode_reward:.2f}\")\n",
    "            print(f\"Goal reached: {info['agents_complete']}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Episode timed out after {max_steps} steps\")\n",
    "        print(f\"Total reward: {episode_reward:.2f}\")\n",
    "    \n",
    "    return episode_reward, info\n",
    "\n",
    "# Run a basic episode\n",
    "print(\"Basic Environment Usage:\")\n",
    "reward, info = run_basic_episode(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization failed: 'module' object is not callable\n",
      "This is normal in headless environments.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "# Visualization example (if matplotlib is available)\n",
    "try:\n",
    "    # Create environment with rendering\n",
    "    vis_env = create_cm_env(difficulty=\"easy\", render_mode=\"rgb_array\")\n",
    "    \n",
    "    obs = vis_env.reset()\n",
    "    \n",
    "    # Get initial rendering\n",
    "    frame = vis_env.render()\n",
    "    \n",
    "    if frame is not None:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(frame)\n",
    "        plt.title(\"CM Environment - Initial State\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Rendering not available in this environment\")\n",
    "    \n",
    "    vis_env.close()\n",
    "except Exception as e:\n",
    "    print(f\"Visualization failed: {e}\")\n",
    "    print(\"This is normal in headless environments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Difficulty Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difficulty Level Comparison:\n",
      "\n",
      "DEBUG difficulty:\n",
      "  Grid size: 5\n",
      "  Agents: 2\n",
      "  Max steps: 50\n",
      "  Box size: 2\n",
      "\n",
      "EASY difficulty:\n",
      "  Grid size: 7\n",
      "  Agents: 2\n",
      "  Max steps: 100\n",
      "  Box size: 2\n",
      "\n",
      "NORMAL difficulty:\n",
      "  Grid size: 7\n",
      "  Agents: 2\n",
      "  Max steps: 100\n",
      "  Box size: 2\n",
      "\n",
      "HARD difficulty:\n",
      "  Grid size: 9\n",
      "  Agents: 3\n",
      "  Max steps: 150\n",
      "  Box size: 2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Compare different difficulty levels\n",
    "difficulties = ['debug', 'easy', 'normal', 'hard']\n",
    "\n",
    "print(\"Difficulty Level Comparison:\")\n",
    "for diff in difficulties:\n",
    "    test_env = create_cm_env(difficulty=diff, render_mode=\"\")\n",
    "    info = test_env.get_env_info()\n",
    "    \n",
    "    print(f\"\\n{diff.upper()} difficulty:\")\n",
    "    print(f\"  Grid size: {info['grid_size']}\")\n",
    "    print(f\"  Agents: {info['n_agents']}\")\n",
    "    print(f\"  Max steps: {info['episode_limit']}\")\n",
    "    print(f\"  Box size: {info['box_size']}\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration with MARL Algorithms\n",
    "\n",
    "The CM environment is designed to work seamlessly with popular MARL algorithms like QMIX, VDN, MADDPG, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARL Integration:\n",
      "  Environment created successfully\n",
      "  Agent IDs: ['agent_0', 'agent_1']\n",
      "  N agents: 2\n",
      "  Observation shape: (8,)\n",
      "  Global state shape: (16,)\n",
      "  MARL integration test passed!\n"
     ]
    }
   ],
   "source": [
    "# Example: Integration with MARL framework\n",
    "try:\n",
    "    from marl.src.envs import create_env_wrapper\n",
    "    \n",
    "    # Create MARL environment wrapper\n",
    "    config = {\n",
    "        'env': {\n",
    "            'name': 'CM',\n",
    "            'difficulty': 'normal',\n",
    "            'global_state_type': 'concat'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    marl_env = create_env_wrapper(config)\n",
    "    \n",
    "    print(\"MARL Integration:\")\n",
    "    print(f\"  Environment created successfully\")\n",
    "    print(f\"  Agent IDs: {marl_env.agent_ids}\")\n",
    "    print(f\"  N agents: {marl_env.n_agents}\")\n",
    "    \n",
    "    # Test MARL environment\n",
    "    obs, _ = marl_env.reset()\n",
    "    global_state = marl_env.get_global_state()\n",
    "    \n",
    "    print(f\"  Observation shape: {list(obs.values())[0].shape}\")\n",
    "    print(f\"  Global state shape: {global_state.shape}\")\n",
    "    \n",
    "    # Run a few steps\n",
    "    for step in range(5):\n",
    "        actions = {agent_id: np.random.randint(0, 5) for agent_id in marl_env.agent_ids}\n",
    "        obs, rewards, dones, infos = marl_env.step(actions)\n",
    "        \n",
    "        if any(dones.values()):\n",
    "            break\n",
    "    \n",
    "    marl_env.close()\n",
    "    print(\"  MARL integration test passed!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"MARL framework not available - this is normal if not installed\")\n",
    "except Exception as e:\n",
    "    print(f\"MARL integration test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The CM environment provides a rich testbed for multi-agent cooperation with the following key characteristics:\n",
    "\n",
    "### Strengths:\n",
    "- **Clear cooperation requirements**: Multiple agents needed for efficient box movement\n",
    "- **Scalable difficulty**: From simple 2-agent to complex 4-agent scenarios\n",
    "- **Rich observations**: Local and global information for CTDE algorithms\n",
    "- **Well-designed rewards**: Balances exploration, cooperation, and goal completion\n",
    "- **MARL-ready**: Seamless integration with popular MARL frameworks\n",
    "\n",
    "### Use Cases:\n",
    "- Testing cooperation mechanisms in MARL\n",
    "- Benchmarking CTDE algorithms\n",
    "- Studying multi-agent coordination\n",
    "- Teaching multi-agent reinforcement learning\n",
    "\n",
    "### Configuration Tips:\n",
    "- Start with `debug` or `easy` difficulty for testing\n",
    "- Use `normal` difficulty for standard benchmarks\n",
    "- Try `hard` difficulty for challenging scenarios\n",
    "- Use CTDE versions (`*_ctde`) for centralized training\n",
    "- Adjust reward parameters in config files for custom requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CM Environment Tutorial completed!\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "env.close()\n",
    "print(\"\\nCM Environment Tutorial completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
