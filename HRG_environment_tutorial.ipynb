{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRG (Heterogeneous Resource Gathering) Environment Tutorial\n",
    "\n",
    "This notebook provides a tutorial for the HRG environment where heterogeneous agents must cooperate to gather resources and deposit them at a base.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The HRG environment features:\n",
    "- **Heterogeneous agents**: Scouts, Workers, and Transporters with different abilities\n",
    "- **Resource collection**: Gold (high value) and Wood (low value)\n",
    "- **Cooperative gameplay**: Multiple agents must work together for efficiency\n",
    "- **Complex observations**: Local vision with agent-specific information\n",
    "- **Fast versions**: Optimized configurations for quick training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "from HRG.env_hrg_ultra_fast import create_hrg_ultra_fast_env\n",
    "from HRG.env_hrg_ultra_fast_ctde import create_hrg_ultra_fast_ctde_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "### Agent Types:\n",
    "- **Scouts**: Fast movement, high vision range, no carrying capacity\n",
    "- **Workers**: Moderate speed, can gather and carry resources\n",
    "- **Transporters**: High carrying capacity, resource transfer specialists\n",
    "\n",
    "### Resources:\n",
    "- **Gold**: High value (10.0), clustered together\n",
    "- **Wood**: Low value (2.0), more scattered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ultra-fast environment (recommended for training)\n",
    "env = create_hrg_ultra_fast_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action and Observation Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:\n",
      "  Actions: 0=Stay, 1=Up, 2=Down, 3=Left, 4=Right, 5=Gather/Deposit/Transfer\n",
      "\n",
      "Observation Space:\n",
      "  worker_0: shape=(24,), range=[0.000, 1.000]\n",
      "    Contains: position, resources, base_info, inventory, energy, etc.\n",
      "  transporter_0: shape=(24,), range=[0.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "# Reset environment and examine spaces\n",
    "obs = env.reset()\n",
    "\n",
    "print(\"Action Space:\")\n",
    "print(f\"  Actions: 0=Stay, 1=Up, 2=Down, 3=Left, 4=Right, 5=Gather/Deposit/Transfer\")\n",
    "\n",
    "print(\"\\nObservation Space:\")\n",
    "for agent_id, observation in obs.items():\n",
    "    print(f\"  {agent_id}: shape={observation.shape}, range=[{observation.min():.3f}, {observation.max():.3f}]\")\n",
    "    \n",
    "    # Show observation breakdown for first agent\n",
    "    if agent_id == list(obs.keys())[0]:\n",
    "        print(f\"    Contains: position, resources, base_info, inventory, energy, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward System\n",
    "\n",
    "### Reward Components:\n",
    "- **Gathering**: 10% × resource value\n",
    "- **Transferring**: 5% × resource value\n",
    "- **Depositing**: 50% × resource value\n",
    "- **Step penalty**: -0.001 (minimal)\n",
    "- **Diversity bonus**: Additional rewards for balanced resource collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode with 2 agents\n",
      "  Step  0: Reward= 0.050, Carried=4, Done=False\n",
      "  Step  5: Reward= 0.033, Carried=4, Done=False\n",
      "  Step 10: Reward=-0.035, Carried=4, Done=False\n",
      "  Step 15: Reward=-0.070, Carried=4, Done=False\n",
      "\n",
      "Episode completed: Total reward = -0.875\n"
     ]
    }
   ],
   "source": [
    "# Run a sample episode\n",
    "def run_episode(env, max_steps=20):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(f\"Starting episode with {len(env.game_state.agents)} agents\")\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Get random actions\n",
    "        actions = {}\n",
    "        for agent_id in env.agent_ids:\n",
    "            avail_actions = env.get_avail_actions(agent_id)\n",
    "            actions[agent_id] = np.random.choice(avail_actions)\n",
    "        \n",
    "        # Execute step\n",
    "        obs, rewards, dones, info = env.step(actions)\n",
    "        \n",
    "        step_reward = list(rewards.values())[0]\n",
    "        total_reward += step_reward\n",
    "        \n",
    "        # Show progress\n",
    "        if step % 5 == 0:\n",
    "            total_resources = sum(len(agent.inventory) for agent in env.game_state.agents.values())\n",
    "            print(f\"  Step {step:2d}: Reward={step_reward:6.3f}, Carried={total_resources}, Done={any(dones.values())}\")\n",
    "        \n",
    "        if any(dones.values()):\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nEpisode completed: Total reward = {total_reward:.3f}\")\n",
    "    return total_reward\n",
    "\n",
    "# Run episode\n",
    "reward = run_episode(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTDE Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTDE Environment:\n",
      "  Global state dimension: 22\n",
      "  Agent IDs: ['worker_0', 'transporter_0']\n",
      "  N agents: 2\n"
     ]
    }
   ],
   "source": [
    "# Create CTDE environment\n",
    "ctde_env = create_hrg_ultra_fast_ctde_env()\n",
    "\n",
    "obs = ctde_env.reset()\n",
    "global_state = ctde_env.get_global_state()\n",
    "\n",
    "print(\"CTDE Environment:\")\n",
    "print(f\"  Global state dimension: {len(global_state)}\")\n",
    "print(f\"  Agent IDs: {ctde_env.agent_ids}\")\n",
    "print(f\"  N agents: {ctde_env.n_agents}\")\n",
    "\n",
    "ctde_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features\n",
    "\n",
    "### 1. Role Specialization\n",
    "- Agents have different capabilities encouraging role emergence\n",
    "- Scouts explore and find resources\n",
    "- Workers gather resources efficiently\n",
    "- Transporters move resources between agents\n",
    "\n",
    "### 2. Fast Training Configurations\n",
    "- **Ultra-fast**: 2 agents, 6x6 grid, minimal complexity\n",
    "- **Fast**: 6 agents, 10x10 grid, moderate complexity\n",
    "- **Normal**: Full 6-agent team, standard complexity\n",
    "\n",
    "### 3. MARL Integration Ready\n",
    "- Compatible with QMIX, VDN, MADDPG algorithms\n",
    "- CTDE interface for centralized training\n",
    "- Unified observation format across all environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HRG Environment Tutorial completed!\n",
      "\n",
      "Key takeaways:\n",
      "  1. Use ultra_fast configuration for quick training\n",
      "  2. Agents naturally specialize into roles\n",
      "  3. Cooperation needed for efficient resource collection\n",
      "  4. Rich reward system encourages balanced strategies\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "env.close()\n",
    "print(\"\\nHRG Environment Tutorial completed!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"  1. Use ultra_fast configuration for quick training\")\n",
    "print(\"  2. Agents naturally specialize into roles\")\n",
    "print(\"  3. Cooperation needed for efficient resource collection\")\n",
    "print(\"  4. Rich reward system encourages balanced strategies\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
