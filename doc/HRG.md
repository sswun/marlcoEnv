## 环境1: **异构资源收集(Heterogeneous Resource Gathering, HRG)**

### **场景描述与设计理念**

**设计目标**: 验证RACGA在**静态角色**场景下的基础能力，侧重显式通信的有效性测试。
**世界观设定**: 在一个未开发的矿区，不同专业的机器人团队需要在有限时间内最大化资源收集效率。地形复杂度中等，部分区域存在障碍物，资源分布呈聚集模式。

#### **空间结构 (10×10 网格世界)**
- **地形类型**:
  - 空地（90个格子）: 可自由通行
  - 障碍物（10个格子）: 随机分布的岩石，不可通行，阻挡视野
  - 基地（1个格子）: 固定在 $(0, 0)$，用于存放资源
- **视野机制**: 
  - 采用**射线投射算法**，障碍物会遮挡视线
  - 不同角色的视野半径体现能力异构性

#### **智能体配置**(固定角色，测试静态RA)

**1. 侦察兵(Scout, 2个)**
- **能力参数**:
  - 视野半径: 5格（曼哈顿距离）
  - 移动速度: 2格/步（可连续移动两次）
  - 负重能力: 0（无法采集资源）
  - 能量消耗: 0.05/步
- **初始位置**: 基地附近 $[(0,1), (1,0)]$
- **角色定位**: 
  - 快速探索未知区域
  - 通过显式通信广播高价值资源位置
  - 需学习**探索-利用平衡**（新区域 vs 已知高价值区）

**2. 工人(Worker, 3个)**
- **能力参数**:
  - 视野半径: 3格
  - 移动速度: 1格/步
  - 负重能力: 2单位（可同时携带2块资源）
  - 采集时间: 2步/单位资源
  - 能量消耗: 0.02/步，0.08/采集动作
- **初始位置**: 基地周围 $[(0,2), (2,0), (1,1)]$
- **角色定位**:
  - 主要生产力来源
  - 需权衡：优先金矿 vs 就近木材 vs 等待侦察信息
  - 需通过隐式观察学习避让运输车（防止路径冲突）

**3. 运输车(Transporter, 1个)**
- **能力参数**:
  - 视野半径: 4格
  - 移动速度: 1.5格/步（可移动1格+0.5概率移动第2格）
  - 负重能力: 5单位（从工人处接收资源）
  - 传输时间: 1步（与工人相邻时自动传输）
  - 能量消耗: 0.03/步，0.1/传输动作
- **初始位置**: 基地 $(0,0)$
- **角色定位**:
  - 物流枢纽，连接采集前线与基地
  - 需学习**最优路径规划**（多点取货问题，类TSP）
  - 需学习**通信时机决策**（何时广播"我在X位置等待传输"）

#### **资源配置与机制**

**金矿(Gold)**
- **数量与分布**: 3个，聚集在地图远端 $(7-9, 7-9)$ 区域
- **价值**: +10分/单位
- **采集难度**: 
  - 每个金矿点包含2单位资源
  - 需要工人连续采集 $2 \times 2 = 4$ 步
- **战略价值**: 高风险高回报，需要侦察兵定位+工人长距离移动

**木材(Wood)**
- **数量与分布**: 10个，均匀散布在地图 $(2-8, 2-8)$ 区域
- **价值**: +2分/单位
- **采集难度**:
  - 每个木材点包含1单位资源
  - 仅需2步采集
- **战略价值**: 低风险稳定收益，适合早期快速获取正反馈

**资源重生机制**
- 被采集的资源点在50步后重生在随机位置
- 防止环境状态耗尽，保持持续挑战

#### **协作需求与通信场景**

**场景1: 侦察-工人协同（显式通信主导）**
- **需求**: 侦察兵发现金矿后，需通过消息告知工人精确坐标
- **挑战**: 
  - 消息编码设计：如何用有限维度表示 $(x,y)$ 坐标？
  - 消息语义学习：工人需从消息中解码位置信息
  - 通信成本博弈：是否每次看到资源都广播？
- **RACGA测试点**: 信号博弈损失应确保"金矿@(7,8)"与"木材@(3,4)"在消息空间中可区分

**场景2: 工人-运输车协同（隐式观察主导）**
- **需求**: 工人观察运输车移动轨迹，预判何时靠近以传输资源
- **挑战**:
  - 隐式行为建模：从运输车的 $(x,y)$ 历史轨迹预测其意图
  - 避免碰撞：多个工人争抢同一运输车时的优先级协调
- **RACGA测试点**: 隐式通道 $b_j^t$ 应包含运输车的位置和速度信息，门控 $w$ 应在此场景降低显式权重

**场景3: 运输车优化（通信门控博弈）**
- **需求**: 运输车需决策何时广播"我已满载，返回基地"
- **挑战**:
  - 动态成本：负重时通信成本应降低（更需要协调）
  - 策略不确定性：路径规划不确定时应更多通信求助
- **RACGA测试点**: 门控机制 $p_i^t$ 应与负重状态正相关

#### **观测空间设计 (部分可观测)**
**每个智能体的观测向量维度: 约60-80维**

**自身状态 (10维)**
```
- 位置: (x, y) ∈ [0,10]²
- 角色ID: one-hot(3)  # [Scout, Worker, Transporter]
- 携带资源: (gold_count, wood_count)
- 能量: energy ∈ [0,100]
- 动作冷却: cooldown ∈ {0,1,2}  # 某些动作需要冷却
```

**视野内实体 (动态维度，最多50维)**
```
对于视野内每个实体(最多8个):
- 相对位置: (Δx, Δy)
- 实体类型: one-hot(5)  # [Scout, Worker, Transporter, Gold, Wood]
- 资源剩余量: count (仅资源点有效)
- 队友负重: (gold, wood) (仅队友有效)
```

**全局信息 (10维，模拟通信接收)**
```
- 最近3条消息的编码向量: 3×d_msg
- 基地距离: dist_to_base
- 剩余时间: time_left / max_time
```

**关键设计考量**:
- **异步观测**: 不同角色的视野半径不同，体现能力异构
- **信息不对称**: 侦察兵能看到工人看不到的资源
- **历史依赖**: 需要RNN/Transformer处理时序信息（如轨迹预测）

#### **动作空间设计**
**离散动作空间 (8个基础动作)**
```
1. 上移动 (Move North)
2. 下移动 (Move South)
3. 左移动 (Move West)
4. 右移动 (Move East)
5. 采集资源 (Gather) - 仅Worker可用
6. 传输资源 (Transfer) - Worker↔Transporter
7. 存放资源 (Deposit) - 仅Transporter在基地可用
8. 等待/观察 (Wait)
```

**通信动作（隐式集成）**
- 不作为独立动作，由门控机制 $p_i^t$ 自动决策
- 通信内容 $m_i^t$ 由策略网络生成

**动作约束 (Masking)**
- Scout不能执行Gather动作
- 仅相邻格子才能Transfer
- 资源满载时不能Gather
- 冷却期内某些动作不可用

#### **奖励函数设计**
**团队奖励 (共享)**
```python
# 主要奖励：存放到基地的资源价值
R_deposit = 10 × gold_deposited + 2 × wood_deposited
# 塑造奖励（引导行为）
R_shape = 0
R_shape += 0.1 × (减少的资源-基地距离总和)  # 引导向基地移动
R_shape += 0.5 × (新发现的资源数)             # 鼓励探索
R_shape += 0.2 × (成功的传输次数)             # 鼓励协作
# 惩罚项
R_penalty = 0
R_penalty -= 0.01 × (总移动距离)               # 轻微能量成本
R_penalty -= 0.05 × (碰撞次数)                 # 惩罚路径冲突
R_penalty -= λ_comm × (通信次数)               # 通信成本，λ_comm ∈ [0.01, 0.1]
# 最终奖励
R = R_deposit + R_shape + R_penalty
```

**奖励时间尺度**
- 即时奖励：采集、传输时的小额奖励（快速反馈）
- 延迟奖励：存放到基地时的大额奖励（长期目标）
- 稀疏奖励：金矿奖励间隔长，通过课程学习缓解

#### **成功/失败条件**
**Episode终止条件**
1. **时间限制**: 200步（约2-3分钟游戏时间）
2. **完美完成**: 所有资源被采集并存放（提前终止，+50额外奖励）
3. **任务失败**: 所有智能体能量耗尽（-20惩罚，实际很少发生）

**评估指标**
```python
success_metrics = {
    'total_score': R,                          # 主要指标
    'gold_collected': gold_count,              # 高价值资源
    'wood_collected': wood_count,              # 基础资源
    'exploration_coverage': visited_cells / 100,  # 探索率
    'comm_efficiency': R / (comm_count + 1),   # 通信效率
    'role_utilization': {                      # 角色利用率
        'scout_discovery_rate': ...,
        'worker_idle_time': ...,
        'transporter_trip_efficiency': ...
    }
}
```

#### **难度梯度设计**
**Easy模式**（热身训练）
- 移除障碍物（视野无遮挡）
- 增加木材数量至15个
- 减少地图至 $8 \times 8$
- 增加episode长度至300步

**Normal模式**（标准评估）
- 上述完整配置

**Hard模式**（挑战测试）
- 增加障碍物至20个（迷宫化）
- 资源动态移动（以随机游走模拟"会跑的猎物"）
- 引入资源竞争：另一支NPC队伍（固定策略）同时采集
- 通信范围限制：仅能与距离≤6的队友通信

#### **RACGA方法适配点详解**
**1. 静态角色嵌入**
```
r_scout = [1, 0, 0, ...]  # 64维预设向量
r_worker = [0, 1, 0, ...]
r_transporter = [0, 0, 1, ...]
```
- 验证点：消融实验移除角色信息后，性能应显著下降
- 可视化：t-SNE图应展示三类智能体的策略清晰分离

**2. 显式通信通道测试**
- 消息维度 $d_{msg} = 16$
- 关键测试：侦察兵的消息编码中，应能提取位置语义
- 实验设计：
  - 记录侦察兵在位置 $(x,y)$ 发现资源时的消息向量
  - 训练线性探测器 $f: m \rightarrow (x,y)$
  - 定位精度应 >80% （容忍±1误差）

**3. 隐式观察通道测试**
- 行为向量 $b_j^t = [x_j, y_j, v_x, v_y, \text{action}]$（5维）
- 关键测试：工人应能从运输车轨迹预测其下一步位置
- 实验设计：
  - 冻结工人的隐式通道（$w=1$），观察避让失败率
  - 对比 $w \in \{0, 0.5, 1\}$ 三种配置

**4. 门控机制验证**
- 记录不同场景下的 $p_i^t$ 统计分布
- 预期行为：
  - 侦察兵发现金矿时：$p \approx 0.8$（高通信需求）
  - 工人正常采集时：$p \approx 0.2$（低通信需求）
  - 运输车满载时：$p \approx 0.6$（需要协调返回）

**5. 信号博弈损失效果**
- 对比损失：$\mathcal{L}_{\text{signal}}$ 确保消息-角色-动作的一致性
- 可视化：
  - 绘制消息空间的PCA投影
  - 不同场景的消息（"发现金矿"vs"请求传输"）应形成清晰聚类
  - 定量指标：聚类轮廓系数 (Silhouette Score) >0.6

#### **与SMAC的对比优势**
| 维度 | SMAC | HRG环境 |
|------|------|---------|
| 角色类型 | 单位类型固定(Marine/Zealot) | 明确功能角色(Scout/Worker) |
| 观测异构 | 视野相似 | 视野差异大(3-5格) |
| 通信需求 | 隐式协调为主 | **显式+隐式双通道** |
| 奖励结构 | 击杀导向 | 资源收集+物流优化 |
| 状态空间 | 高维连续(单位属性多) | 低维离散(易于分析) |
| 收敛难度 | 极高(需百万步) | 中等(预计20万步) |

**关键优势**: HRG环境专为测试RACGA设计，每个组件都有对应的测试场景，而SMAC更侧重通用RL算法测试。