# 协作搬运游戏Collaborative Moving - 完整设计与实现

## 完整游戏设计文档

### 环境参数
```python
- 网格大小: 7x7
- 智能体数量: 2-4个（可配置）
- 盒子: 1个（2x2大小，占4个格子）
- 目标区域: 2x2大小
- 最大步数: 100步
```

### 详细规则

#### 1. 搬运机制
- **1个智能体推**: 盒子移动概率 50%
- **2个智能体协作**: 盒子移动概率 75%
- **3个智能体协作**: 盒子移动概率 90%
- **4个智能体协作**: 盒子移动 100%

#### 2. 协作判定
智能体需要在盒子的**不同侧面**才算有效协作：
- 上侧、下侧、左侧、右侧
- 同一侧多个智能体只算1个

#### 3. 奖励函数
```
- 时间惩罚: -0.01
- 盒子到达目标: +10
- 盒子中心接近目标: +0.5 * 距离减少
- 有效协作奖励: +0.02 * 协作人数
- 碰撞惩罚: -0.1（智能体互相碰撞）
```

## 完整代码实现

```python
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from typing import List, Tuple, Dict
import matplotlib.pyplot as plt
import matplotlib.patches as patches

class CooperativeBoxPushingEnv(gym.Env):
    """
    多智能体协作搬运环境
    支持2-4个智能体协作将盒子推到目标位置
    """
    
    metadata = {'render_modes': ['human', 'rgb_array'], 'render_fps': 4}
    
    # 动作定义
    ACTIONS = {
        0: np.array([0, 0]),   # 停留
        1: np.array([-1, 0]),  # 上
        2: np.array([1, 0]),   # 下
        3: np.array([0, -1]),  # 左
        4: np.array([0, 1])    # 右
    }
    
    def __init__(self, 
                 grid_size: int = 7,
                 n_agents: int = 2,
                 max_steps: int = 100,
                 render_mode: str = None):
        """
        初始化环境
        
        Args:
            grid_size: 网格大小
            n_agents: 智能体数量 (2-4)
            max_steps: 最大步数
            render_mode: 渲染模式
        """
        super().__init__()
        
        assert 2 <= n_agents <= 4, "智能体数量必须在2-4之间"
        assert grid_size >= 7, "网格大小至少为7x7"
        
        self.grid_size = grid_size
        self.n_agents = n_agents
        self.max_steps = max_steps
        self.render_mode = render_mode
        
        # 盒子和目标大小（占据2x2格子）
        self.box_size = 2
        self.goal_size = 2
        
        # 动作和观测空间
        self.n_actions = 5
        self.action_space = spaces.Discrete(self.n_actions)
        
        # 每个智能体的观测: [自己x, 自己y, 盒子中心x, 盒子中心y, 目标中心x, 目标中心y, 
        #                    其他智能体相对位置...]
        obs_dim = 6 + (n_agents - 1) * 2
        self.observation_space = spaces.Box(
            low=0, high=grid_size-1, 
            shape=(obs_dim,), 
            dtype=np.float32
        )
        
        # 状态
        self.agent_positions = None
        self.box_position = None  # 左上角位置
        self.goal_position = None  # 左上角位置
        self.current_step = 0
        
        # 渲染
        self.fig = None
        self.ax = None
        
    def reset(self, seed=None, options=None):
        """重置环境"""
        super().reset(seed=seed)
        
        self.current_step = 0
        
        # 初始化盒子位置（中心区域）
        box_x = self.grid_size // 2 - 1
        box_y = self.grid_size // 2 - 1
        self.box_position = np.array([box_x, box_y], dtype=np.float32)
        
        # 初始化目标位置（随机但不与盒子重叠）
        while True:
            goal_x = np.random.randint(0, self.grid_size - self.goal_size)
            goal_y = np.random.randint(0, self.grid_size - self.goal_size)
            self.goal_position = np.array([goal_x, goal_y], dtype=np.float32)
            
            # 确保目标与初始盒子位置有足够距离
            if np.linalg.norm(self.goal_position - self.box_position) > 3:
                break
        
        # 初始化智能体位置（在盒子周围）
        self.agent_positions = []
        occupied = set()
        
        # 将盒子占据的位置加入occupied
        for i in range(self.box_size):
            for j in range(self.box_size):
                occupied.add((box_x + i, box_y + j))
        
        # 在盒子周围放置智能体
        box_adjacent = [
            (box_x - 1, box_y), (box_x - 1, box_y + 1),  # 上方
            (box_x + 2, box_y), (box_x + 2, box_y + 1),  # 下方
            (box_x, box_y - 1), (box_x + 1, box_y - 1),  # 左方
            (box_x, box_y + 2), (box_x + 1, box_y + 2),  # 右方
        ]
        
        valid_positions = [
            pos for pos in box_adjacent 
            if 0 <= pos[0] < self.grid_size and 0 <= pos[1] < self.grid_size
        ]
        
        np.random.shuffle(valid_positions)
        
        for i in range(self.n_agents):
            if i < len(valid_positions):
                pos = valid_positions[i]
            else:
                # 如果周围位置不够，随机放置
                while True:
                    pos = (np.random.randint(0, self.grid_size),
                          np.random.randint(0, self.grid_size))
                    if pos not in occupied:
                        break
            
            self.agent_positions.append(np.array(pos, dtype=np.float32))
            occupied.add(pos)
        
        obs = self._get_obs()
        info = self._get_info()
        
        return obs, info
    
    def step(self, actions: List[int]):
        """
        执行一步
        
        Args:
            actions: 每个智能体的动作列表
            
        Returns:
            observations, rewards, terminated, truncated, info
        """
        assert len(actions) == self.n_agents, "动作数量必须等于智能体数量"
        
        self.current_step += 1
        
        # 1. 移动智能体
        new_positions = []
        collision_penalty = 0
        
        for i, action in enumerate(actions):
            new_pos = self.agent_positions[i] + self.ACTIONS[action]
            
            # 边界检查
            new_pos = np.clip(new_pos, 0, self.grid_size - 1)
            
            new_positions.append(new_pos)
        
        # 检查智能体之间的碰撞
        position_set = {}
        for i, pos in enumerate(new_positions):
            pos_tuple = tuple(pos)
            if pos_tuple in position_set:
                # 碰撞，两个智能体都不移动
                collision_penalty -= 0.1
                new_positions[i] = self.agent_positions[i]
                new_positions[position_set[pos_tuple]] = self.agent_positions[position_set[pos_tuple]]
            else:
                position_set[pos_tuple] = i
        
        # 检查是否与盒子碰撞（不能进入盒子内部）
        for i, pos in enumerate(new_positions):
            if self._is_in_box(pos, self.box_position):
                new_positions[i] = self.agent_positions[i]
        
        self.agent_positions = new_positions
        
        # 2. 计算协作推力
        push_sides = self._get_push_sides()
        n_cooperating = len(push_sides)
        
        # 3. 移动盒子
        box_moved = False
        old_box_pos = self.box_position.copy()
        
        if n_cooperating > 0:
            # 根据协作人数决定移动概率
            move_probs = {1: 0.3, 2: 0.7, 3: 0.9, 4: 1.0}
            move_prob = move_probs.get(n_cooperating, 0)
            
            if np.random.random() < move_prob:
                # 计算推力方向（多数方向）
                push_direction = self._calculate_push_direction(push_sides)
                new_box_pos = self.box_position + push_direction
                
                # 检查盒子是否出界
                if (0 <= new_box_pos[0] <= self.grid_size - self.box_size and
                    0 <= new_box_pos[1] <= self.grid_size - self.box_size):
                    self.box_position = new_box_pos
                    box_moved = True
        
        # 4. 计算奖励
        reward = -0.01  # 时间惩罚
        reward += collision_penalty
        
        # 协作奖励
        if n_cooperating > 1:
            reward += 0.02 * n_cooperating
        
        # 距离奖励
        old_distance = self._box_goal_distance(old_box_pos)
        new_distance = self._box_goal_distance(self.box_position)
        distance_reward = (old_distance - new_distance) * 0.5
        reward += distance_reward
        
        # 5. 检查是否完成
        terminated = self._is_goal_reached()
        if terminated:
            reward += 10.0
        
        truncated = self.current_step >= self.max_steps
        
        # 6. 获取观测和信息
        obs = self._get_obs()
        info = self._get_info()
        info['n_cooperating'] = n_cooperating
        info['box_moved'] = box_moved
        
        # 所有智能体获得相同的团队奖励
        rewards = [reward] * self.n_agents
        
        return obs, rewards, terminated, truncated, info
    
    def _get_push_sides(self) -> List[str]:
        """获取正在推盒子的智能体所在的侧面"""
        sides = []
        box_x, box_y = self.box_position
        
        for pos in self.agent_positions:
            x, y = pos
            
            # 上侧
            if x == box_x - 1 and box_y <= y < box_y + self.box_size:
                if 'top' not in sides:
                    sides.append('top')
            # 下侧
            elif x == box_x + self.box_size and box_y <= y < box_y + self.box_size:
                if 'bottom' not in sides:
                    sides.append('bottom')
            # 左侧
            elif y == box_y - 1 and box_x <= x < box_x + self.box_size:
                if 'left' not in sides:
                    sides.append('left')
            # 右侧
            elif y == box_y + self.box_size and box_x <= x < box_x + self.box_size:
                if 'right' not in sides:
                    sides.append('right')
        
        return sides
    
    def _calculate_push_direction(self, sides: List[str]) -> np.ndarray:
        """根据推力侧面计算盒子移动方向"""
        direction = np.array([0, 0])
        
        if 'top' in sides and 'bottom' not in sides:
            direction[0] -= 1
        elif 'bottom' in sides and 'top' not in sides:
            direction[0] += 1
            
        if 'left' in sides and 'right' not in sides:
            direction[1] -= 1
        elif 'right' in sides and 'left' not in sides:
            direction[1] += 1
        
        return direction
    
    def _is_in_box(self, pos: np.ndarray, box_pos: np.ndarray) -> bool:
        """检查位置是否在盒子内部"""
        x, y = pos
        box_x, box_y = box_pos
        return (box_x <= x < box_x + self.box_size and 
                box_y <= y < box_y + self.box_size)
    
    def _box_goal_distance(self, box_pos: np.ndarray) -> float:
        """计算盒子中心到目标中心的距离"""
        box_center = box_pos + self.box_size / 2
        goal_center = self.goal_position + self.goal_size / 2
        return np.linalg.norm(box_center - goal_center)
    
    def _is_goal_reached(self) -> bool:
        """检查盒子是否到达目标"""
        return np.array_equal(self.box_position, self.goal_position)
    
    def _get_obs(self) -> List[np.ndarray]:
        """获取所有智能体的观测"""
        observations = []
        
        box_center = self.box_position + self.box_size / 2
        goal_center = self.goal_position + self.goal_size / 2
        
        for i in range(self.n_agents):
            obs = []
            
            # 自己的位置
            obs.extend(self.agent_positions[i])
            
            # 盒子中心位置
            obs.extend(box_center)
            
            # 目标中心位置
            obs.extend(goal_center)
            
            # 其他智能体的相对位置
            for j in range(self.n_agents):
                if i != j:
                    relative_pos = self.agent_positions[j] - self.agent_positions[i]
                    obs.extend(relative_pos)
            
            observations.append(np.array(obs, dtype=np.float32))
        
        return observations
    
    def _get_info(self) -> Dict:
        """获取额外信息"""
        return {
            'current_step': self.current_step,
            'box_position': self.box_position.copy(),
            'goal_position': self.goal_position.copy(),
            'distance_to_goal': self._box_goal_distance(self.box_position),
            'agent_positions': [pos.copy() for pos in self.agent_positions]
        }
    
    def render(self):
        """渲染环境"""
        if self.render_mode is None:
            return
        
        if self.fig is None:
            self.fig, self.ax = plt.subplots(figsize=(8, 8))
            plt.ion()
        
        self.ax.clear()
        self.ax.set_xlim(-0.5, self.grid_size - 0.5)
        self.ax.set_ylim(-0.5, self.grid_size - 0.5)
        self.ax.set_aspect('equal')
        self.ax.grid(True, alpha=0.3)
        self.ax.set_xticks(range(self.grid_size))
        self.ax.set_yticks(range(self.grid_size))
        
        # 绘制目标区域
        goal_rect = patches.Rectangle(
            (self.goal_position[1] - 0.4, self.goal_position[0] - 0.4),
            self.goal_size + 0.8, self.goal_size + 0.8,
            linewidth=2, edgecolor='green', facecolor='lightgreen', alpha=0.3
        )
        self.ax.add_patch(goal_rect)
        
        # 绘制盒子
        box_rect = patches.Rectangle(
            (self.box_position[1] - 0.3, self.box_position[0] - 0.3),
            self.box_size + 0.6, self.box_size + 0.6,
            linewidth=2, edgecolor='brown', facecolor='orange', alpha=0.7
        )
        self.ax.add_patch(box_rect)
        
        # 绘制智能体
        colors = ['red', 'blue', 'purple', 'cyan']
        for i, pos in enumerate(self.agent_positions):
            circle = patches.Circle(
                (pos[1], pos[0]), 0.3,
                color=colors[i], alpha=0.8
            )
            self.ax.add_patch(circle)
            self.ax.text(pos[1], pos[0], str(i+1), 
                        ha='center', va='center', color='white', fontweight='bold')
        
        # 标题
        distance = self._box_goal_distance(self.box_position)
        self.ax.set_title(f'Step: {self.current_step} | Distance: {distance:.2f}', 
                         fontsize=14)
        
        # 反转y轴使(0,0)在左上角
        self.ax.invert_yaxis()
        
        plt.draw()
        plt.pause(0.1)
        
        if self.render_mode == 'rgb_array':
            self.fig.canvas.draw()
            data = np.frombuffer(self.fig.canvas.tostring_rgb(), dtype=np.uint8)
            data = data.reshape(self.fig.canvas.get_width_height()[::-1] + (3,))
            return data
    
    def close(self):
        """关闭环境"""
        if self.fig is not None:
            plt.close(self.fig)
            self.fig = None
            self.ax = None


# ==================== 测试代码 ====================

def test_random_policy(n_agents=2, episodes=5, render=True):
    """测试随机策略"""
    env = CooperativeBoxPushingEnv(
        grid_size=7,
        n_agents=n_agents,
        max_steps=100,
        render_mode='human' if render else None
    )
    
    for episode in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        done = False
        step = 0
        
        print(f"\n{'='*50}")
        print(f"Episode {episode + 1}")
        print(f"{'='*50}")
        
        while not done and step < 100:
            # 随机动作
            actions = [env.action_space.sample() for _ in range(n_agents)]
            
            obs, rewards, terminated, truncated, info = env.step(actions)
            
            total_reward += rewards[0]
            done = terminated or truncated
            step += 1
            
            if render:
                env.render()
            
            if step % 20 == 0:
                print(f"Step {step}: Distance={info['distance_to_goal']:.2f}, "
                      f"Cooperating={info.get('n_cooperating', 0)}")
        
        print(f"\nEpisode finished!")
        print(f"Total steps: {step}")
        print(f"Total reward: {total_reward:.2f}")
        print(f"Goal reached: {terminated}")
        print(f"Final distance: {info['distance_to_goal']:.2f}")
    
    env.close()


if __name__ == "__main__":
    print("测试 2 智能体协作搬运环境")
    test_random_policy(n_agents=2, episodes=3, render=True)
    
    # print("\n\n测试 4 智能体协作搬运环境")
    # test_random_policy(n_agents=4, episodes=2, render=True)
```

## 使用示例

### 1. 基础使用
```python
from box_pushing_env import CooperativeBoxPushingEnv

# 创建环境
env = CooperativeBoxPushingEnv(n_agents=3)

# 重置环境
obs, info = env.reset()

# 执行动作
actions = [1, 2, 0]  # 智能体1上，智能体2下，智能体3停留
obs, rewards, terminated, truncated, info = env.step(actions)

# 渲染
env.render()
```

### 2. 训练接口
```python
# 适配主流MARL库的接口
def get_env_info(env):
    return {
        'n_agents': env.n_agents,
        'n_actions': env.n_actions,
        'obs_shape': env.observation_space.shape[0],
        'episode_limit': env.max_steps
    }
```

## 特性总结

✅ **支持2-4个智能体**  
✅ **明确的协作机制**（人数越多，成功率越高）  
✅ **轻量级实现**（易于调试）  
✅ **可视化渲染**  
✅ **标准Gym接口**  
✅ **适合IQL/QMIX/MAPPO等算法**  

这个环境既简单又有效，非常适合验证多智能体强化学习算法的收敛性！