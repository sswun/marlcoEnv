# HRG环境详解：观测空间、动作空间与奖励机制

## 1. 观测空间（80维）详细解析

观测空间定义在 `env_hrg.py` 的 `_get_observation` 方法中，共80维，结构如下：

### 1.1 自身状态（10维）

| 维度 | 特征 | 描述 |
|------|------|------|
| 2 | 位置信息 | `position.x/grid_size`, `position.y/grid_size` - 归一化的坐标 |
| 3 | 角色类型 | one-hot编码 `[侦察兵, 工人, 运输车]` |
| 2 | 库存状态 | `gold/10.0`, `wood/10.0` - 归一化的资源数量 |
| 2 | 能量和冷却 | `energy/100.0`, `action_cooldown/2.0` - 能量和动作冷却时间 |
| 1 | 基地距离 | 到基地的曼哈顿距离归一化值 |

### 1.2 时间信息（1维）

- **剩余时间比例**：`1.0 - (current_step / max_steps)`
- 提供任务进度信息，帮助智能体规划长期策略

### 1.3 可见实体信息（最多50维）

最多观察10个实体，每个实体5维信息：

| 维度 | 特征 | 描述 |
|------|------|------|
| 2 | 相对位置 | `(pos.x - agent.x)/vision_range`, `(pos.y - agent.y)/vision_range` |
| 3 | 实体类型 | 
  - **其他智能体**：3维one-hot编码 `[侦察兵, 工人, 运输车]`
  - **资源**：2维one-hot编码 `[金矿, 木材]` + 1维资源数量 |

**关键特性**：
- 不同角色视野范围不同：侦察兵(5格) > 运输车(4格) > 工人(3格)
- 体现能力异构性，鼓励角色专业化

### 1.4 通信历史（10维）

最近3条消息的简化编码（基于发送者类型）：
- **侦察兵消息**：0.3
- **工人消息**：0.6  
- **运输车消息**：0.9

**设计哲学**：
- 模拟真实通信中的"信道质量"
- 避免直接传递完整语义，增加学习难度
- 鼓励智能体从有限信息中推断意图

## 2. 动作空间（8维）详细解析

动作空间定义在 `core.py` 中，包含8个离散动作：

### 2.1 移动动作（4个）

| 动作ID | 名称 | 描述 |
|--------|------|------|
| 0 | MOVE_NORTH | 向北移动 |
| 1 | MOVE_SOUTH | 向南移动 |
| 2 | MOVE_WEST | 向西移动 |
| 3 | MOVE_EAST | 向东移动 |

**物理约束**：
- 侦察兵：速度2.0（可连续移动两次）
- 工人：速度1.0
- 运输车：速度1.5（可移动1格+0.5概率移动第2格）

### 2.2 交互动作（4个）

| 动作ID | 名称 | 描述 |
|--------|------|------|
| 4 | GATHER | 采集当前位置的资源<br>• 工人专属动作，侦察兵无法采集<br>• 需要连续执行多个回合才能完成采集<br>• 采集时间：金矿4回合，木材2回合 |
| 5 | TRANSFER | 与相邻智能体转移资源<br>• 工人→运输车：转移采集到的资源<br>• 运输车→工人：通常为空操作<br>• 只有在相邻位置且有对应类型的智能体时才能执行 |
| 6 | DEPOSIT | 在基地存放资源<br>• 运输车专属动作，只有运输车可以存放资源<br>• 必须在基地位置(0,0)才能执行<br>• 获得资源价值的50%作为奖励 |
| 7 | WAIT | 等待/跳过回合 |

### 2.3 角色限制

不同角色有不同的动作限制：

| 动作 | 侦察兵 | 工人 | 运输车 |
|------|--------|------|--------|
| GATHER | ❌ | ✅ | ❌ |
| DEPOSIT | ❌ | ❌ | ✅ |
| TRANSFER | ✅ | ✅ | ✅ |
| MOVE | ✅ | ✅ | ✅ |

**设计优势**：
- 强制角色分化，避免所有智能体选择相同策略
- 模拟现实世界中的专业分工
- 促进团队协作需求

## 3. 奖励机制详细解析

奖励系统设计在 `env_hrg.py` 的 `_calculate_team_reward` 方法中，包含个人奖励和团队奖励。

### 3.1 个人动作奖励

#### 1. 移动奖励（_move_agent）
- **成功移动**：0.0（无奖励，仅消耗能量）
- **无效移动**：-0.1（撞墙或移动到无效位置）
- **能量消耗**：根据角色类型消耗不同能量
  - 侦察兵：0.05/步
  - 工人：0.02/步
  - 运输车：0.03/步

#### 2. 采集奖励（_gather_resource）
- **采集完成**：获得资源价值的10%
  - 金矿：10.0 × 0.1 = 1.0
  - 木材：2.0 × 0.1 = 0.2
- **采集过程中**：0.0（需要多回合完成）
- **能量消耗**：工人采集时消耗0.08能量

#### 3. 转移奖励（_transfer_resource）
- **成功转移**：获得转移资源价值的5%
  - 金矿：10.0 × 0.05 = 0.5
  - 木材：2.0 × 0.05 = 0.1
- **能量消耗**：转移时消耗0.1能量

#### 4. 存放奖励（_deposit_resources）
- **成功存放**：获得资源价值的50%
  - 金矿：10.0 × 0.5 = 5.0
  - 木材：2.0 × 0.5 = 1.0
- 这是最大的单次奖励，鼓励资源成功运回基地

### 3.2 团队奖励（_calculate_team_reward）

| 奖励类型 | 值 | 描述 |
|---------|-----|------|
| 时间惩罚 | -0.01/步 | 鼓励快速完成任务 |
| 资源多样性奖励 | +0.1 (金矿), +0.05 (木材) | 第一次存放特定资源时给予额外奖励 |
| 协作奖励 | 动态计算 | 基于团队整体效率的附加奖励 |

### 3.3 角色能力差异

| 角色 | 优势 | 限制 | 能量消耗 |
|------|------|------|----------|
| **侦察兵** | 视野范围5，移动速度2.0 | 无法采集，携带容量0 | 移动0.05 |
| **工人** | 可采集，携带容量2 | 视野范围3，移动速度1.0 | 移动0.02，采集0.08 |
| **运输车** | 可存放资源，携带容量5，移动速度1.5 | 无法采集，视野范围4 | 移动0.03，转移0.1 |

## 4. 奖励设计理念

### 4.1 核心原则

1. **鼓励协作**
   - 转移奖励(0.5) < 存放奖励(5.0)，形成"采集→转移→存放"的价值链
   - 团队共享最终奖励，强化集体目标

2. **效率导向**
   - 每步-0.01的时间惩罚，促使智能体快速行动
   - 能量消耗机制防止无限循环行为

3. **角色专精**
   - 不同角色有不同的最优策略
   - 侦察兵应专注于探索和通信
   - 工人应专注于资源采集
   - 运输车应专注于物流调度

4. **目标明确**
   - 最终目标是收集并存放资源到基地
   - 奖励信号清晰指向核心任务

### 4.2 多层次奖励结构

```
+-----------------------+
|      最终目标         |
|  资源存放至基地       |
+-----------------------+
           |
           v
+-----------------------+
|     中间目标          |
|  资源采集与转移       |
+-----------------------+
           |
           v
+-----------------------+
|     基础行为          |
|  探索、移动、避障     |
+-----------------------+
```

## 5. 总结

HRG环境是一个典型的异构多智能体协作环境，具有以下特点：

- **80维观测空间**：提供了丰富的环境信息，包括自身状态、时间信息、可见实体和通信历史
- **8维动作空间**：支持复杂的决策，包含移动、采集、转移和存放等交互动作
- **精心设计的奖励机制**：通过多层次奖励引导智能体学习高效协作策略

**环境优势**：
1. **角色分化需求高**：强制不同角色承担不同职责
2. **通信必要性强**：侦察兵发现资源后必须通知工人
3. **物流链条完整**：从探索到采集再到运输形成完整闭环
4. **挑战性适中**：既不过于简单也不过于复杂，适合算法测试

这个环境为测试多智能体协作算法提供了一个理想的平台，特别是对于验证RACGA等强调角色动态性和双通道通信的方法。