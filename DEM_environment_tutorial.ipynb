{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEM (Defense, Escort, and Movement) Environment Tutorial\n",
    "\n",
    "This notebook provides a comprehensive tutorial for the DEM environment, where agents must protect a VIP character while escorting them from a starting position to a target location, defending against various threats.\n",
    "\n",
    "## Table of Contents\n",
    "1. Environment Overview\n",
    "2. Task Description\n",
    "3. Action Space\n",
    "4. Observation Space\n",
    "5. CTDE Global State Space\n",
    "6. Reward System\n",
    "7. Usage Examples\n",
    "8. Integration with MARL Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Overview\n",
    "\n",
    "The DEM environment is a complex multi-agent defense and escort scenario where agents must protect a VIP (Very Important Person) while guiding them to safety. This environment tests coordination, defense, and strategic planning capabilities.\n",
    "\n",
    "**Key Features:**\n",
    "- VIP protection and escort mechanics\n",
    "- Dynamic threat spawning (rushers and shooters)\n",
    "- Terrain effects (rivers, forests)\n",
    "- Role emergence possibilities (guards, vanguards, snipers)\n",
    "- Rich observation and reward systems\n",
    "- Configurable difficulty levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from DEM.env_dem import create_dem_env\n",
    "from DEM.env_dem_ctde import create_dem_ctde_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task Description\n",
    "\n",
    "### Objective\n",
    "Protect and escort a VIP from their starting position to a target location while defending against dynamically spawning threats.\n",
    "\n",
    "### Core Mechanics\n",
    "\n",
    "**VIP Behavior:**\n",
    "- Autonomous movement toward target\n",
    "- Limited vision range and movement speed\n",
    "- Can take damage from threats\n",
    "- Must reach target alive for success\n",
    "\n",
    "**Threat Types:**\n",
    "1. **Rushers**: Close-range melee attackers\n",
    "   - High movement speed (1 cell per step)\n",
    "   - Short attack range (1 cell)\n",
    "   - Moderate damage (8 HP)\n",
    "   - Fast attack cooldown (1 step)\n",
    "\n",
    "2. **Shooters**: Long-range ranged attackers\n",
    "   - No movement (stationary)\n",
    "   - Long attack range (5 cells)\n",
    "   - High damage (15 HP)\n",
    "   - Slow attack cooldown (3 steps)\n",
    "\n",
    "**Terrain Effects:**\n",
    "- **Rivers**: Impede movement (cannot move through)\n",
    "- **Forests**: Provide damage reduction (30% less damage taken)\n",
    "\n",
    "### Episode Termination\n",
    "- VIP reaches target location (success)\n",
    "- VIP dies (failure)\n",
    "- Maximum steps reached\n",
    "- Time limit exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Information:\n",
      "  n_agents: 3\n",
      "  agent_ids: ['agent_0', 'agent_1', 'agent_2']\n",
      "  action_space: Discrete(10)\n",
      "  observation_space: Box(-1.0, 1.0, (59,), float32)\n",
      "  global_state_dim: 41\n",
      "  max_steps: 200\n",
      "  episode_limit: 200\n",
      "  obs_shape: 59\n",
      "  n_actions: 10\n",
      "  state_shape: 41\n",
      "\n",
      "Environment Configuration:\n",
      "  Grid size: 12x12\n",
      "  Number of agents: 3\n",
      "  Max steps: 200\n",
      "  VIP initial HP: 60\n",
      "  Agent HP: 50\n",
      "  Max threats: 5\n",
      "  Difficulty levels: ['easy', 'normal', 'hard']\n"
     ]
    }
   ],
   "source": [
    "# Create environment and demonstrate basic functionality\n",
    "env = create_dem_env(difficulty=\"normal\", render_mode=\"\")\n",
    "\n",
    "print(\"Environment Information:\")\n",
    "env_info = env.get_env_info()\n",
    "for key, value in env_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nEnvironment Configuration:\")\n",
    "print(f\"  Grid size: {env.config.grid_size}x{env.config.grid_size}\")\n",
    "print(f\"  Number of agents: {env.config.num_agents}\")\n",
    "print(f\"  Max steps: {env.config.max_steps}\")\n",
    "print(f\"  VIP initial HP: {env.config.vip_hp}\")\n",
    "print(f\"  Agent HP: {env.config.agent_hp}\")\n",
    "print(f\"  Max threats: {env.config.max_threats}\")\n",
    "print(f\"  Difficulty levels: ['easy', 'normal', 'hard']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Action Space\n",
    "\n",
    "Each agent has 9 discrete actions combining movement and combat:\n",
    "\n",
    "| Action ID | Action Name | Description |\n",
    "|-----------|-------------|-------------|\n",
    "| 0 | STAY | Agent remains in current position |\n",
    "| 1 | UP | Agent moves one grid cell up |\n",
    "| 2 | DOWN | Agent moves one grid cell down |\n",
    "| 3 | LEFT | Agent moves one grid cell left |\n",
    "| 4 | RIGHT | Agent moves one grid cell right |\n",
    "| 5 | ATTACK_UP | Attack one cell up |\n",
    "| 6 | ATTACK_DOWN | Attack one cell down |\n",
    "| 7 | ATTACK_LEFT | Attack one cell left |\n",
    "| 8 | ATTACK_RIGHT | Attack one cell right |\n",
    "\n",
    "### Action Constraints\n",
    "- Movement actions are invalid if target cell is occupied or blocked\n",
    "- Attack actions are valid only if there's an enemy in range\n",
    "- Attacks have cooldown periods\n",
    "- Agents cannot move into rivers or VIP positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:\n",
      "  Number of actions: 10\n",
      "  Action space: Discrete(10)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate action space\n",
    "obs = env.reset()\n",
    "\n",
    "print(\"Action Space:\")\n",
    "print(f\"  Number of actions: {env_info['n_actions']}\")\n",
    "print(f\"  Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Observation Space\n",
    "\n",
    "Each agent receives a rich local observation containing information about their surroundings:\n",
    "\n",
    "**Vector format (length varies with configuration):**\n",
    "- **Self information (5 values)**:\n",
    "  - Agent position (x, y)\n",
    "  - Current HP\n",
    "  - Attack cooldown status\n",
    "  - Last action taken\n",
    "\n",
    "- **VIP information (4 values)**:\n",
    "  - VIP position (x, y)\n",
    "  - VIP current HP\n",
    "  - VIP target position (x, y)\n",
    "\n",
    "- **Threat information (variable, max 20 values)**:\n",
    "  - Up to 5 nearest threats\n",
    "  - Each threat: position (x, y), type, HP\n",
    "\n",
    "- **Other agents information (variable, max 20 values)**:\n",
    "  - Up to 5 nearest friendly agents\n",
    "  - Each agent: position (x, y), HP\n",
    "\n",
    "- **Terrain information (25 values)**:\n",
    "  - 5x5 grid around agent showing terrain types\n",
    "  - 0: empty, 1: river, 2: forest, 3: VIP, 4: threat\n",
    "\n",
    "- **Goal information (1 value)**:\n",
    "  - Distance to VIP target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space:\n",
      "\n",
      "  agent_0:\n",
      "    Shape: (59,)\n",
      "    Min: -0.083, Max: 1.000\n",
      "    Self info: pos=(0.0, 0.1), hp=1.0, cooldown=0.0, last_action=0.0\n",
      "    VIP info: pos=(0.1, 0.9), hp=0.0, target=(1.0, 1.0)\n",
      "    Distance to target: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate observation space\n",
    "obs = env.reset()\n",
    "\n",
    "print(\"Observation Space:\")\n",
    "for i, (agent_id, observation) in enumerate(obs.items()):\n",
    "    print(f\"\\n  {agent_id}:\")\n",
    "    print(f\"    Shape: {observation.shape}\")\n",
    "    print(f\"    Min: {observation.min():.3f}, Max: {observation.max():.3f}\")\n",
    "    \n",
    "    # Decode observation components\n",
    "    idx = 0\n",
    "    print(f\"    Self info: pos=({observation[idx]:.1f}, {observation[idx+1]:.1f}), hp={observation[idx+2]:.1f}, cooldown={observation[idx+3]:.1f}, last_action={observation[idx+4]:.1f}\")\n",
    "    idx += 5\n",
    "    \n",
    "    print(f\"    VIP info: pos=({observation[idx]:.1f}, {observation[idx+1]:.1f}), hp={observation[idx+2]:.1f}, target=({observation[idx+3]:.1f}, {observation[idx+4]:.1f})\")\n",
    "    idx += 5\n",
    "    \n",
    "    if len(observation) > idx + 2:\n",
    "        print(f\"    Distance to target: {observation[-1]:.1f}\")\n",
    "    \n",
    "    if i == 0:  # Show details for first agent only\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CTDE Global State Space\n",
    "\n",
    "For Centralized Training with Decentralized Execution (CTDE), the environment provides a comprehensive global state containing information about all entities in the environment.\n",
    "\n",
    "**Global State Components:**\n",
    "- **All agent positions and HP** (n_agents × 3 values)\n",
    "- **VIP position, HP, and target** (5 values)\n",
    "- **All threat positions, types, and HP** (variable, up to max_threats × 4 values)\n",
    "- **Terrain information** (grid_size × grid_size values)\n",
    "- **Time and step information** (2 values)\n",
    "- **Communication history** (if enabled)\n",
    "\n",
    "**Global State Types:**\n",
    "- `concat`: Concatenation of all information (default)\n",
    "- `mean`: Mean pooling of agent observations\n",
    "- `max`: Max pooling of agent observations\n",
    "- `attention`: Attention-based aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTDE Environment:\n",
      "  Global state dimension: 41\n",
      "  Global state sample: [0.08333334 0.08333334 1.         0.         0.         0.08333334\n",
      " 1.         0.         0.08333334 0.         1.         0.\n",
      " 0.16666667 0.08333334 1.        ]...\n",
      "\n",
      "State breakdown:\n",
      "  Agents: 3\n",
      "  Max threats: 5\n",
      "  Grid size: 12x12\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate CTDE environment\n",
    "ctde_env = create_dem_ctde_env(difficulty=\"normal\", global_state_type=\"concat\")\n",
    "\n",
    "obs = ctde_env.reset()\n",
    "global_state = ctde_env.get_global_state()\n",
    "\n",
    "print(\"CTDE Environment:\")\n",
    "print(f\"  Global state dimension: {len(global_state)}\")\n",
    "print(f\"  Global state sample: {global_state[:15]}...\")  # Show first 15 values\n",
    "\n",
    "print(\"\\nState breakdown:\")\n",
    "print(f\"  Agents: {ctde_env.config.num_agents}\")\n",
    "print(f\"  Max threats: {ctde_env.config.max_threats}\")\n",
    "print(f\"  Grid size: {ctde_env.config.grid_size}x{ctde_env.config.grid_size}\")\n",
    "\n",
    "ctde_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reward System\n",
    "\n",
    "The reward system is designed to encourage VIP protection, escort efficiency, and strategic agent behavior.\n",
    "\n",
    "**Primary Rewards:**\n",
    "1. **VIP Reach Target**: +50.0 (main success reward)\n",
    "2. **VIP Death**: -30.0 (major failure penalty)\n",
    "3. **VIP Progress**: +0.2 per grid unit closer to target\n",
    "4. **Threat Killed**: +3.0 per enemy eliminated\n",
    "5. **VIP Damage**: -0.1 per HP point lost\n",
    "6. **Agent Death**: -3.0 per friendly agent lost\n",
    "\n",
    "**Role Emergence Rewards:**\n",
    "- **Guard Adjacent**: +0.05 for being next to VIP\n",
    "- **Guard Missing Penalty**: -0.02 if no agent near VIP\n",
    "- **Body Block**: +0.5 for blocking threats from VIP\n",
    "- **Vanguard Ahead**: +0.05 for being ahead of VIP\n",
    "- **Vanguard Missing Penalty**: -0.02 if no agent ahead\n",
    "- **Long Range Kill**: +1.0 for kills from ≥6 units away\n",
    "- **Formation Rewards**: ±0.02 for good/bad agent spacing\n",
    "\n",
    "**Movement Penalties:**\n",
    "- **Collision**: -0.05 for agent-agent collisions\n",
    "- **Invalid Action**: -0.1 for impossible actions\n",
    "\n",
    "**Difficulty Scaling:**\n",
    "- **Easy**: Reduced threat spawn rate, higher VIP/agent HP\n",
    "- **Normal**: Balanced challenge level\n",
    "- **Hard**: Increased threat spawn rate, lower HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward System Configuration:\n",
      "  VIP reach target: 50.0\n",
      "  VIP death: -30.0\n",
      "  VIP progress: 0.2 per unit\n",
      "  Threat killed: 3.0\n",
      "  VIP damage: -0.1 per HP\n",
      "  Agent death: -3.0\n",
      "  Body block: 0.5\n",
      "  Long range kill: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate reward system\n",
    "print(\"Reward System Configuration:\")\n",
    "print(f\"  VIP reach target: {env.config.reward_vip_reach_target}\")\n",
    "print(f\"  VIP death: {env.config.reward_vip_death}\")\n",
    "print(f\"  VIP progress: {env.config.reward_vip_progress} per unit\")\n",
    "print(f\"  Threat killed: {env.config.reward_threat_killed}\")\n",
    "print(f\"  VIP damage: {env.config.reward_vip_damage} per HP\")\n",
    "print(f\"  Agent death: {env.config.reward_agent_death}\")\n",
    "print(f\"  Body block: {env.config.reward_body_block}\")\n",
    "print(f\"  Long range kill: {env.config.reward_long_range_kill}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Difficulty Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difficulty Level Comparison:\n",
      "\n",
      "EASY difficulty:\n",
      "  Grid size: 10x10\n",
      "  VIP HP: 80\n",
      "  Agent HP: 60\n",
      "  Max threats: 3\n",
      "  Threat spawn interval: 12\n",
      "  Rusher probability: 0.4\n",
      "\n",
      "NORMAL difficulty:\n",
      "  Grid size: 12x12\n",
      "  VIP HP: 60\n",
      "  Agent HP: 50\n",
      "  Max threats: 5\n",
      "  Threat spawn interval: 8\n",
      "  Rusher probability: 0.6\n",
      "\n",
      "HARD difficulty:\n",
      "  Grid size: 12x12\n",
      "  VIP HP: 40\n",
      "  Agent HP: 40\n",
      "  Max threats: 8\n",
      "  Threat spawn interval: 6\n",
      "  Rusher probability: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Compare different difficulty levels\n",
    "difficulties = ['easy', 'normal', 'hard']\n",
    "\n",
    "print(\"Difficulty Level Comparison:\")\n",
    "for diff in difficulties:\n",
    "    test_env = create_dem_env(difficulty=diff, render_mode=\"\")\n",
    "    \n",
    "    print(f\"\\n{diff.upper()} difficulty:\")\n",
    "    print(f\"  Grid size: {test_env.config.grid_size}x{test_env.config.grid_size}\")\n",
    "    print(f\"  VIP HP: {test_env.config.vip_hp}\")\n",
    "    print(f\"  Agent HP: {test_env.config.agent_hp}\")\n",
    "    print(f\"  Max threats: {test_env.config.max_threats}\")\n",
    "    print(f\"  Threat spawn interval: {test_env.config.threat_spawn_base_interval}\")\n",
    "    print(f\"  Rusher probability: {test_env.config.rusher_probability:.1f}\")\n",
    "    \n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration with MARL Algorithms\n",
    "\n",
    "The DEM environment is designed for seamless integration with popular MARL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARL Integration:\n",
      "  Environment created successfully\n",
      "  Agent IDs: ['agent_0', 'agent_1', 'agent_2']\n",
      "  N agents: 3\n",
      "  Observation shape: (59,)\n",
      "  Global state shape: (41,)\n",
      "  MARL integration test passed!\n"
     ]
    }
   ],
   "source": [
    "# Example: Integration with MARL framework\n",
    "try:\n",
    "    from marl.src.envs import create_env_wrapper\n",
    "    \n",
    "    # Create MARL environment wrapper\n",
    "    config = {\n",
    "        'env': {\n",
    "            'name': 'DEM',\n",
    "            'difficulty': 'normal',\n",
    "            'global_state_type': 'concat'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    marl_env = create_env_wrapper(config)\n",
    "    \n",
    "    print(\"MARL Integration:\")\n",
    "    print(f\"  Environment created successfully\")\n",
    "    print(f\"  Agent IDs: {marl_env.agent_ids}\")\n",
    "    print(f\"  N agents: {marl_env.n_agents}\")\n",
    "    \n",
    "    # Test MARL environment\n",
    "    obs, _ = marl_env.reset()\n",
    "    global_state = marl_env.get_global_state()\n",
    "    \n",
    "    print(f\"  Observation shape: {list(obs.values())[0].shape}\")\n",
    "    print(f\"  Global state shape: {global_state.shape}\")\n",
    "    \n",
    "    # Run a few steps\n",
    "    for step in range(5):\n",
    "        actions = {agent_id: np.random.randint(0, 9) for agent_id in marl_env.agent_ids}\n",
    "        obs, rewards, dones, infos = marl_env.step(actions)\n",
    "        \n",
    "        if any(dones.values()):\n",
    "            print(f\"  Episode completed at step {step+1}\")\n",
    "            break\n",
    "    \n",
    "    marl_env.close()\n",
    "    print(\"  MARL integration test passed!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"MARL framework not available - this is normal if not installed\")\n",
    "except Exception as e:\n",
    "    print(f\"MARL integration test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The DEM environment provides a complex and realistic multi-agent defense scenario with the following key characteristics:\n",
    "\n",
    "### Strengths:\n",
    "- **Rich dynamics**: VIP escort, threat spawning, terrain effects\n",
    "- **Role emergence**: Natural emergence of guard, vanguard, and sniper roles\n",
    "- **Strategic depth**: Multiple objectives (protection, offense, positioning)\n",
    "- **Scalable difficulty**: Easy to hard configurations\n",
    "- **Comprehensive observations**: Local and global information for CTDE\n",
    "- **Well-designed rewards**: Balance multiple objectives\n",
    "\n",
    "### Use Cases:\n",
    "- Testing multi-agent coordination under pressure\n",
    "- Studying role emergence in teams\n",
    "- Benchmarking defensive MARL algorithms\n",
    "- Research on hierarchical agent behaviors\n",
    "- Training cooperative defense strategies\n",
    "\n",
    "### Configuration Tips:\n",
    "- Start with `easy` difficulty to understand basic mechanics\n",
    "- Use `normal` for standard benchmarks and research\n",
    "- Try `hard` for challenging multi-objective scenarios\n",
    "- Adjust threat spawn rates for custom difficulty curves\n",
    "- Modify reward weights to emphasize different objectives\n",
    "- Use CTDE versions for centralized training algorithms\n",
    "\n",
    "### Key Challenges:\n",
    "- Balancing protection vs. escort objectives\n",
    "- Coordinating against diverse threat types\n",
    "- Optimizing agent positioning and formations\n",
    "- Managing limited resources (agent HP, attack cooldowns)\n",
    "- Adapting to dynamic threat spawning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEM Environment Tutorial completed!\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "env.close()\n",
    "print(\"\\nDEM Environment Tutorial completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
