{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM (Collaborative Moving) 环境使用教程\n",
    "\n",
    "本教程将介绍如何使用CM环境进行多智能体强化学习实验。\n",
    "\n",
    "## 目录\n",
    "1. [环境简介](#环境简介)\n",
    "2. [基础使用](#基础使用)\n",
    "3. [CTDE兼容性](#ctde兼容性)\n",
    "4. [可视化功能](#可视化功能)\n",
    "5. [配置管理](#配置管理)\n",
    "6. [性能测试](#性能测试)\n",
    "7. [高级功能](#高级功能)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境简介\n",
    "\n",
    "CM是一个协作推箱子环境，智能体需要协作将箱子推到目标位置：\n",
    "- **协作机制**：箱子需要多个智能体从不同方向推动才能成功移动\n",
    "- **成功概率**：推动成功的概率随协作智能体数量增加而提高\n",
    "- **团队奖励**：所有智能体共享团队奖励，鼓励协作\n",
    "\n",
    "环境的目标是测试多智能体协作算法的收敛性和性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM环境详解：观测空间、动作空间与奖励机制\n",
    "\n",
    "## 1. 观测空间详细解析\n",
    "\n",
    "观测空间定义在 `env_cm.py` 的 `_get_observations` 方法中，维度为 `6 + 2*(n_agents-1)`，结构如下：\n",
    "\n",
    "### 1.1 自身位置信息（2维）\n",
    "\n",
    "| 维度 | 特征 | 描述 |\n",
    "|------|------|------|\n",
    "| 2 | 位置信息 | `position.x`, `position.y` - 智能体的网格坐标 |\n",
    "\n",
    "### 1.2 箱子中心位置（2维）\n",
    "\n",
    "| 维度 | 特征 | 描述 |\n",
    "|------|------|------|\n",
    "| 2 | 箱子中心 | `box_center_x`, `box_center_y` - 箱子的中心坐标 |\n",
    "\n",
    "### 1.3 目标中心位置（2维）\n",
    "\n",
    "| 维度 | 特征 | 描述 |\n",
    "|------|------|------|\n",
    "| 2 | 目标中心 | `goal_center_x`, `goal_center_y` - 目标区域的中心坐标 |\n",
    "\n",
    "### 1.4 其他智能体相对位置（2*(n_agents-1)维）\n",
    "\n",
    "对于每个其他智能体，提供2维相对位置信息：\n",
    "\n",
    "| 维度 | 特征 | 描述 |\n",
    "|------|------|------|\n",
    "| 2 | 相对位置 | `other_agent.x - self.x`, `other_agent.y - self.y` |\n",
    "\n",
    "**观测空间总维度计算**：\n",
    "```\n",
    "obs_dim = 6 (自身+箱子+目标) + 2 * (n_agents - 1) (其他智能体)\n",
    "```\n",
    "\n",
    "例如：\n",
    "- 2个智能体：6 + 2 = 8维\n",
    "- 3个智能体：6 + 4 = 10维\n",
    "- 4个智能体：6 + 6 = 12维\n",
    "\n",
    "### 1.5 观测归一化\n",
    "\n",
    "如果配置了 `normalize_observations=True`，所有观测值会除以网格大小进行归一化到[0,1]范围。\n",
    "\n",
    "**设计优势**：\n",
    "- 简洁明了：只包含必要的位置信息\n",
    "- 协作导向：包含其他智能体位置便于协作\n",
    "- 易于学习：低维观测空间便于算法收敛\n",
    "\n",
    "## 2. 动作空间（5维）详细解析\n",
    "\n",
    "动作空间定义在 `core.py` 的 `ActionType` 枚举中，包含5个离散动作：\n",
    "\n",
    "| 动作ID | 名称 | 描述 |\n",
    "|--------|------|------|\n",
    "| 0 | STAY | 原地等待 |\n",
    "| 1 | MOVE_UP | 向上移动 |\n",
    "| 2 | MOVE_DOWN | 向下移动 |\n",
    "| 3 | MOVE_LEFT | 向左移动 |\n",
    "| 4 | MOVE_RIGHT | 向右移动 |\n",
    "\n",
    "### 2.1 动作约束\n",
    "\n",
    "- **边界约束**：智能体不能移出网格边界\n",
    "- **箱子碰撞**：智能体不能移动到箱子内部\n",
    "- **智能体碰撞**：多个智能体不能占据同一位置\n",
    "\n",
    "### 2.2 动作掩码\n",
    "\n",
    "环境提供 `get_avail_actions()` 方法返回当前可用的动作列表，考虑边界约束。\n",
    "\n",
    "**设计特点**：\n",
    "- 简单直观：5个基本动作易于理解和学习\n",
    "- 安全约束：动作掩码确保无效动作不会被执行\n",
    "- 协作友好：移动动作便于智能体协调位置\n",
    "\n",
    "## 3. 奖励机制详细解析\n",
    "\n",
    "奖励系统设计在 `env_cm.py` 的 `_calculate_rewards` 方法中，采用团队奖励机制。\n",
    "\n",
    "### 3.1 基础奖励组成\n",
    "\n",
    "#### 1. 时间惩罚\n",
    "```\n",
    "time_penalty = -0.01 (默认值)\n",
    "```\n",
    "- 每步都会扣除，鼓励智能体快速完成任务\n",
    "- 不同难度配置有不同的时间惩罚值\n",
    "\n",
    "#### 2. 碰撞惩罚\n",
    "```\n",
    "agent_collision_penalty = -0.1\n",
    "box_collision_penalty = -0.05\n",
    "```\n",
    "- 智能体之间的碰撞惩罚较重\n",
    "- 撞墙（边界碰撞）惩罚较轻\n",
    "\n",
    "#### 3. 协作奖励\n",
    "```\n",
    "cooperation_reward * len(pushing_sides)\n",
    "```\n",
    "- 当多个智能体同时推动箱子时给予奖励\n",
    "- 推动边数越多，奖励越高\n",
    "- 鼓励多角度协作推动\n",
    "\n",
    "#### 4. 距离奖励\n",
    "```\n",
    "-distance * distance_reward_scale * 0.01\n",
    "```\n",
    "- 箱子与目标的欧几里得距离\n",
    "- 距离越近，惩罚越小（即奖励越大）\n",
    "- 引导智能体向目标移动\n",
    "\n",
    "#### 5. 箱子移动奖励\n",
    "```\n",
    "box_move_reward_scale = 0.5\n",
    "```\n",
    "- 箱子成功移动时给予的即时奖励\n",
    "- 为成功的协作行为提供正向反馈\n",
    "\n",
    "#### 6. 目标达成奖励\n",
    "```\n",
    "goal_reached_reward = 10.0 (默认值)\n",
    "```\n",
    "- 箱子到达目标位置时的大额奖励\n",
    "- 提供明确的目标导向信号\n",
    "\n",
    "### 3.2 推动成功概率\n",
    "\n",
    "箱子推动的成功概率取决于参与推动的智能体数量：\n",
    "\n",
    "| 推动智能体数 | 成功概率（普通） | 成功概率（简单） | 成功概率（困难） |\n",
    "|-------------|-----------------|-----------------|-----------------|\n",
    "| 1个智能体 | 50% | 70% | 30% |\n",
    "| 2个智能体 | 75% | 90% | 60% |\n",
    "| 3个智能体 | 90% | 100% | 85% |\n",
    "| 4个智能体 | 100% | 100% | 100% |\n",
    "\n",
    "### 3.3 团队奖励机制\n",
    "\n",
    "所有智能体获得相同的奖励值：\n",
    "```\n",
    "rewards = {agent_id: total_reward for agent_id in agent_ids}\n",
    "```\n",
    "\n",
    "**设计理念**：\n",
    "- **强调协作**：个人成功不如团队成功重要\n",
    "- **公平分配**：避免竞争行为，鼓励合作\n",
    "- **共同目标**：所有智能体为同一目标努力\n",
    "\n",
    "## 4. 推动机制详解\n",
    "\n",
    "### 4.1 推动条件\n",
    "\n",
    "智能体必须位于箱子的侧面才能推动：\n",
    "- **上侧**：箱子上方相邻位置\n",
    "- **下侧**：箱子下方相邻位置\n",
    "- **左侧**：箱子左侧相邻位置\n",
    "- **右侧**：箱子右侧相邻位置\n",
    "\n",
    "### 4.2 推动方向计算\n",
    "\n",
    "推动方向由推动的侧面决定：\n",
    "- 只有上方推动 → 向上推\n",
    "- 只有下方推动 → 向下推\n",
    "- 只有左侧推动 → 向左推\n",
    "- 只有右侧推动 → 向右推\n",
    "- 相对侧面推动 → 抵消，不移动\n",
    "\n",
    "### 4.3 协作推动策略\n",
    "\n",
    "最有效的推动策略：\n",
    "1. **双智能体协作**：两个智能体从相对方向推动\n",
    "2. **多智能体协作**：三个或四个智能体从不同角度推动\n",
    "3. **位置协调**：智能体需要协调到合适的推动位置\n",
    "\n",
    "## 5. 配置参数对奖励的影响\n",
    "\n",
    "### 5.1 难度级别差异\n",
    "\n",
    "| 参数 | 简单模式 | 普通模式 | 困难模式 |\n",
    "|------|----------|----------|----------|\n",
    "| 时间惩罚 | -0.005 | -0.01 | -0.015 |\n",
    "| 协作奖励 | 0.03 | 0.02 | 0.015 |\n",
    "| 目标奖励 | 15.0 | 10.0 | 8.0 |\n",
    "| 距离奖励系数 | 0.8 | 0.5 | 0.3 |\n",
    "\n",
    "### 5.2 特殊配置\n",
    "\n",
    "- **合作测试配置**：大幅提高协作收益，强制多智能体合作\n",
    "- **单智能体配置**：移除协作奖励，测试单智能体性能\n",
    "- **多智能体配置**：增加碰撞惩罚，测试更多智能体的协调\n",
    "\n",
    "## 6. 总结\n",
    "\n",
    "CM环境是一个典型的多智能体协作环境，具有以下特点：\n",
    "\n",
    "- **低维观测空间**：6 + 2*(n_agents-1)维，信息简洁有效\n",
    "- **简单动作空间**：5个离散动作，易于学习\n",
    "- **精心设计的奖励机制**：多层次奖励引导智能体学习协作策略\n",
    "- **概率性推动机制**：增加环境挑战性和现实性\n",
    "\n",
    "**环境优势**：\n",
    "1. **协作需求明确**：单智能体几乎无法完成任务\n",
    "2. **算法友好**：简单的状态和动作空间便于算法收敛\n",
    "3. **可配置性强**：多种难度和专用配置\n",
    "4. **评估直观**：任务成功与否一目了然\n",
    "\n",
    "这个环境为测试多智能体协作算法提供了一个理想的平台，特别适合验证QMIX、VDN等强调团队协作的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM环境导入成功！\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加环境路径\n",
    "import sys\n",
    "sys.path.append(\"..\")  # 将上级目录添加到系统路径\n",
    "\n",
    "# 导入CM环境\n",
    "from Env.CM import create_cm_env, create_cm_ctde_env, create_cm_env_from_config\n",
    "from Env.CM.config import get_config_by_name\n",
    "from Env.CM.renderer import MatplotlibRenderer\n",
    "\n",
    "print(\"CM环境导入成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境信息:\n",
      "- 智能体数量: 2\n",
      "- 智能体ID: ['agent_0', 'agent_1']\n",
      "- 网格大小: 7x7\n",
      "- 最大步数: 100\n",
      "- 箱子大小: 2x2\n",
      "- 观察空间维度: 8\n",
      "- 动作空间维度: 5\n"
     ]
    }
   ],
   "source": [
    "# 创建基础CM环境\n",
    "env = create_cm_env(difficulty=\"normal\", render_mode=\"\")\n",
    "\n",
    "print(f\"环境信息:\")\n",
    "print(f\"- 智能体数量: {env.n_agents}\")\n",
    "print(f\"- 智能体ID: {env.agent_ids}\")\n",
    "print(f\"- 网格大小: {env.config.grid_size}x{env.config.grid_size}\")\n",
    "print(f\"- 最大步数: {env.config.max_steps}\")\n",
    "print(f\"- 箱子大小: {env.config.box_size}x{env.config.box_size}\")\n",
    "print(f\"- 观察空间维度: {env.observation_space.shape[0]}\")\n",
    "print(f\"- 动作空间维度: {env.n_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始观察信息:\n",
      "- agent_0: 观察形状=(8,), 数据类型=float32\n",
      "  最小值=-0.143, 最大值=0.643\n"
     ]
    }
   ],
   "source": [
    "observations = env.reset()\n",
    "\n",
    "print(\"初始观察信息:\")\n",
    "for agent_id, obs in observations.items():\n",
    "    print(f\"- {agent_id}: 观察形状={obs.shape}, 数据类型={obs.dtype}\")\n",
    "    print(f\"  最小值={obs.min():.3f}, 最大值={obs.max():.3f}\")\n",
    "    break  # 只显示第一个智能体的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始执行回合...\n",
      "步数 10: 平均奖励 = -0.026, 距离目标 = 3.16\n",
      "步数 20: 平均奖励 = -0.030, 距离目标 = 4.00\n",
      "步数 30: 平均奖励 = -0.031, 距离目标 = 4.12\n",
      "步数 40: 平均奖励 = 0.469, 距离目标 = 4.12\n",
      "步数 50: 平均奖励 = -0.030, 距离目标 = 4.00\n",
      "\n",
      "回合结果:\n",
      "- agent_0: 总奖励 = 5.499\n",
      "- agent_1: 总奖励 = 5.499\n",
      "- 团队平均奖励: 5.499\n",
      "- 最终距离目标: 4.00\n",
      "- 目标达成: False\n"
     ]
    }
   ],
   "source": [
    "# 执行一个简单的回合\n",
    "total_rewards = {agent_id: 0 for agent_id in env.agent_ids}\n",
    "step_count = 0\n",
    "\n",
    "print(\"开始执行回合...\")\n",
    "\n",
    "while step_count < 50:  # 限制步数用于演示\n",
    "    # 随机选择动作\n",
    "    actions = {}\n",
    "    for agent_id in env.agent_ids:\n",
    "        # 获取可用动作\n",
    "        avail_actions = env.get_avail_actions(agent_id)\n",
    "        action = np.random.choice(avail_actions)\n",
    "        actions[agent_id] = action\n",
    "    \n",
    "    # 执行动作\n",
    "    observations, rewards, done, info = env.step(actions)\n",
    "    \n",
    "    # 累积奖励\n",
    "    for agent_id, reward in rewards.items():\n",
    "        total_rewards[agent_id] += reward\n",
    "    \n",
    "    step_count += 1\n",
    "    \n",
    "    # 打印进度\n",
    "    if step_count % 10 == 0:\n",
    "        avg_reward = np.mean(list(rewards.values()))\n",
    "        print(f\"步数 {step_count}: 平均奖励 = {avg_reward:.3f}, 距离目标 = {info['distance_to_goal']:.2f}\")\n",
    "    \n",
    "    # 检查是否结束\n",
    "    for agent_id, done_flag in done.items():\n",
    "        if done_flag:\n",
    "            print(f\"- {agent_id} 已完成\")\n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"\\n回合结果:\")\n",
    "for agent_id, total_reward in total_rewards.items():\n",
    "    print(f\"- {agent_id}: 总奖励 = {total_reward:.3f}\")\n",
    "print(f\"- 团队平均奖励: {np.mean(list(total_rewards.values())):.3f}\")\n",
    "print(f\"- 最终距离目标: {info['distance_to_goal']:.2f}\")\n",
    "print(f\"- 目标达成: {info['agents_complete']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境已关闭\n"
     ]
    }
   ],
   "source": [
    "# 关闭环境\n",
    "env.close()\n",
    "print(\"环境已关闭\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTDE兼容性\n",
    "\n",
    "CM环境提供了CTDE（集中式训练分布式执行）包装器，支持QMIX、VDN等算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试全局状态类型: concat\n",
      "  - Info中全局状态形状: (16,)\n",
      "\n",
      "测试全局状态类型: mean\n",
      "  - Info中全局状态形状: (8,)\n",
      "\n",
      "测试全局状态类型: max\n",
      "  - Info中全局状态形状: (8,)\n",
      "\n",
      "测试全局状态类型: attention\n",
      "  - Info中全局状态形状: (24,)\n"
     ]
    }
   ],
   "source": [
    "# 测试不同类型的全局状态表示\n",
    "global_state_types = [\"concat\", \"mean\", \"max\", \"attention\"]\n",
    "\n",
    "for state_type in global_state_types:\n",
    "    print(f\"\\n测试全局状态类型: {state_type}\")\n",
    "    \n",
    "    # 创建CTDE环境\n",
    "    ctde_env = create_cm_ctde_env(\n",
    "        \"easy_ctde\",\n",
    "        global_state_type=state_type\n",
    "    )\n",
    "    \n",
    "    # 重置环境\n",
    "    observations = ctde_env.reset()\n",
    "    \n",
    "    # 执行一步并检查info中的全局状态\n",
    "    actions = {agent_id: 0 for agent_id in ctde_env.agent_ids}\n",
    "    obs, rewards, dones, infos = ctde_env.step(actions)\n",
    "    \n",
    "    if 'global_state' in infos:\n",
    "        info_state = infos['global_state']\n",
    "        print(f\"  - Info中全局状态形状: {info_state.shape}\")\n",
    "    \n",
    "    ctde_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTDE环境信息:\n",
      "- n_agents: 2\n",
      "- agent_ids: ['agent_0', 'agent_1']\n",
      "- n_actions: 5\n",
      "- obs_dims: {'agent_0': 8, 'agent_1': 8}\n",
      "- act_dims: {'agent_0': 5, 'agent_1': 5}\n",
      "- episode_limit: 100\n",
      "- grid_size: 7\n",
      "- box_size: 2\n",
      "- global_state_dim: 16\n",
      "- global_state_type: concat\n"
     ]
    }
   ],
   "source": [
    "# 获取CTDE环境信息\n",
    "ctde_env = create_cm_ctde_env(\"normal_ctde\")\n",
    "env_info = ctde_env.get_env_info()\n",
    "\n",
    "print(\"CTDE环境信息:\")\n",
    "for key, value in env_info.items():\n",
    "    print(f\"- {key}: {value}\")\n",
    "\n",
    "ctde_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化功能\n",
    "\n",
    "CM环境提供了多种可视化选项，包括实时可视化和静态图表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22339/2542603725.py:12: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# 创建环境并生成静态可视化\n",
    "env = create_cm_env(difficulty=\"normal\", render_mode=\"rgb_array\")\n",
    "observations = env.reset()\n",
    "\n",
    "# 渲染环境\n",
    "rgb_array = env.render()\n",
    "if rgb_array is not None:\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(rgb_array)\n",
    "    plt.title(f\"CM环境初始状态\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"无法生成图像，可能是matplotlib不可用\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成环境状态快照...\n",
      "agent_0 获得奖励 -0.02288854381999832\n",
      "agent_1 获得奖励 -0.02288854381999832\n",
      "agent_0 获得奖励 -0.02288854381999832\n",
      "agent_1 获得奖励 -0.02288854381999832\n",
      "agent_0 获得奖励 -0.02288854381999832\n",
      "agent_1 获得奖励 -0.02288854381999832\n",
      "agent_0 获得奖励 -0.02288854381999832\n",
      "agent_1 获得奖励 -0.02288854381999832\n",
      "agent_0 获得奖励 0.479\n",
      "agent_1 获得奖励 0.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22339/1036361539.py:34: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# 动态演示：执行几步并可视化\n",
    "env = create_cm_env(difficulty=\"easy\", render_mode=\"rgb_array\")\n",
    "observations = env.reset()\n",
    "\n",
    "print(\"生成环境状态快照...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('CM环境状态演化', fontsize=16)\n",
    "\n",
    "for step in range(6):\n",
    "    # 渲染当前状态\n",
    "    rgb_array = env.render()\n",
    "    \n",
    "    if rgb_array is not None:\n",
    "        ax = axes[step // 3, step % 3]\n",
    "        ax.imshow(rgb_array)\n",
    "        ax.set_title(f'步骤 {step}: ')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # 执行随机动作\n",
    "    if step < 5:  # 不在最后一步执行\n",
    "        actions = {agent_id: np.random.choice(env.get_avail_actions(agent_id)) \n",
    "                  for agent_id in env.agent_ids}\n",
    "        observations, rewards, done, info = env.step(actions)\n",
    "        \n",
    "        for agent_id, reward in rewards.items():\n",
    "            print(f'{agent_id} 获得奖励 {reward}')\n",
    "        \n",
    "        for agent_id, done_ in done.items():\n",
    "            if done_:\n",
    "                print(f'{agent_id}  Episode done')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置管理\n",
    "\n",
    "CM环境提供了丰富的配置选项，支持不同的实验设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "难度配置: easy\n",
      "  - 网格大小: 7x7\n",
      "  - 智能体数量: 2\n",
      "  - 最大步数: 100\n",
      "  - 箱子大小: 2x2\n",
      "  - 时间惩罚: -0.005\n",
      "  - 目标奖励: 15.0\n",
      "  - 推动成功概率: {1: 0.7, 2: 0.9, 3: 1.0, 4: 1.0}\n",
      "\n",
      "难度配置: normal\n",
      "  - 网格大小: 7x7\n",
      "  - 智能体数量: 2\n",
      "  - 最大步数: 100\n",
      "  - 箱子大小: 2x2\n",
      "  - 时间惩罚: -0.01\n",
      "  - 目标奖励: 10.0\n",
      "  - 推动成功概率: {1: 0.5, 2: 0.75, 3: 0.9, 4: 1.0}\n",
      "\n",
      "难度配置: hard\n",
      "  - 网格大小: 9x9\n",
      "  - 智能体数量: 3\n",
      "  - 最大步数: 150\n",
      "  - 箱子大小: 2x2\n",
      "  - 时间惩罚: -0.015\n",
      "  - 目标奖励: 8.0\n",
      "  - 推动成功概率: {1: 0.3, 2: 0.6, 3: 0.85, 4: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# 测试不同难度配置\n",
    "difficulties = [\"easy\", \"normal\", \"hard\"]\n",
    "\n",
    "for difficulty in difficulties:\n",
    "    print(f\"\\n难度配置: {difficulty}\")\n",
    "    \n",
    "    env = create_cm_env(difficulty=difficulty)\n",
    "    \n",
    "    print(f\"  - 网格大小: {env.config.grid_size}x{env.config.grid_size}\")\n",
    "    print(f\"  - 智能体数量: {env.config.n_agents}\")\n",
    "    print(f\"  - 最大步数: {env.config.max_steps}\")\n",
    "    print(f\"  - 箱子大小: {env.config.box_size}x{env.config.box_size}\")\n",
    "    print(f\"  - 时间惩罚: {env.config.time_penalty}\")\n",
    "    print(f\"  - 目标奖励: {env.config.goal_reached_reward}\")\n",
    "    print(f\"  - 推动成功概率: {env.config.push_success_probs}\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有可用配置:\n",
      "  - easy\n",
      "  - normal\n",
      "  - hard\n",
      "  - debug\n",
      "  - cooperation_test\n",
      "  - single_agent\n",
      "  - multi_agent\n",
      "  - easy_ctde\n",
      "  - normal_ctde\n",
      "  - hard_ctde\n",
      "\n",
      "专用配置: cooperation_test\n",
      "  - 智能体数量: 3\n",
      "  - 网格大小: 7x7\n",
      "  - 协作奖励: 0.05\n",
      "  - 推动成功概率: {1: 0.2, 2: 0.7, 3: 0.95, 4: 1.0}\n",
      "\n",
      "专用配置: single_agent\n",
      "  - 智能体数量: 1\n",
      "  - 网格大小: 5x5\n",
      "  - 协作奖励: 0.0\n",
      "  - 推动成功概率: {1: 0.8, 2: 1.0, 3: 1.0, 4: 1.0}\n",
      "\n",
      "专用配置: multi_agent\n",
      "  - 智能体数量: 4\n",
      "  - 网格大小: 9x9\n",
      "  - 协作奖励: 0.025\n",
      "  - 推动成功概率: {1: 0.3, 2: 0.6, 3: 0.85, 4: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# 测试专用配置\n",
    "from Env.CM.config import list_available_configs\n",
    "\n",
    "print(\"所有可用配置:\")\n",
    "configs = list_available_configs()\n",
    "for config_name in configs:\n",
    "    print(f\"  - {config_name}\")\n",
    "\n",
    "# 测试特殊配置\n",
    "special_configs = [\"cooperation_test\", \"single_agent\", \"multi_agent\"]\n",
    "\n",
    "for config_name in special_configs:\n",
    "    if config_name in configs:\n",
    "        print(f\"\\n专用配置: {config_name}\")\n",
    "        \n",
    "        try:\n",
    "            config = get_config_by_name(config_name)\n",
    "            print(f\"  - 智能体数量: {config.n_agents}\")\n",
    "            print(f\"  - 网格大小: {config.grid_size}x{config.grid_size}\")\n",
    "            print(f\"  - 协作奖励: {config.cooperation_reward}\")\n",
    "            print(f\"  - 推动成功概率: {config.push_success_probs}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  - 错误: {e}\")\n",
    "    else:\n",
    "        print(f\"\\n配置 {config_name} 不存在\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自定义配置:\n",
      "- 网格大小: 8\n",
      "- 智能体数量: 3\n",
      "- 最大步数: 120\n",
      "- 协作奖励: 0.04\n",
      "- 推动成功概率: {1: 0.4, 2: 0.8, 3: 1.0, 4: 1.0}\n",
      "\n",
      "自定义环境创建成功！\n",
      "环境信息: {'n_agents': 3, 'agent_ids': ['agent_0', 'agent_1', 'agent_2'], 'n_actions': 5, 'obs_dims': {'agent_0': 10, 'agent_1': 10, 'agent_2': 10}, 'act_dims': {'agent_0': 5, 'agent_1': 5, 'agent_2': 5}, 'episode_limit': 120, 'grid_size': 8, 'box_size': 2}\n"
     ]
    }
   ],
   "source": [
    "# 自定义配置示例\n",
    "from Env.CM.config import CMConfig\n",
    "\n",
    "# 创建自定义配置\n",
    "custom_config = CMConfig(\n",
    "    grid_size=8,\n",
    "    n_agents=3,\n",
    "    max_steps=120,\n",
    "    box_size=2,\n",
    "    goal_size=2,\n",
    "    push_success_probs={1: 0.4, 2: 0.8, 3: 1.0, 4: 1.0},\n",
    "    cooperation_reward=0.04,\n",
    "    time_penalty=-0.008,\n",
    "    goal_reached_reward=12.0,\n",
    "    distance_reward_scale=0.6,\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "print(\"自定义配置:\")\n",
    "print(f\"- 网格大小: {custom_config.grid_size}\")\n",
    "print(f\"- 智能体数量: {custom_config.n_agents}\")\n",
    "print(f\"- 最大步数: {custom_config.max_steps}\")\n",
    "print(f\"- 协作奖励: {custom_config.cooperation_reward}\")\n",
    "print(f\"- 推动成功概率: {custom_config.push_success_probs}\")\n",
    "\n",
    "# 创建环境\n",
    "custom_env = create_cm_env_from_config(custom_config)\n",
    "\n",
    "print(f\"\\n自定义环境创建成功！\")\n",
    "print(f\"环境信息: {custom_env.get_env_info()}\")\n",
    "\n",
    "custom_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性能测试\n",
    "\n",
    "评估环境的执行性能，确保适合强化学习训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "性能测试结果:\n",
      "- 总时间: 0.031秒\n",
      "- 平均每步时间: 0.0001秒\n",
      "- 每秒步数: 6378.6\n",
      "- 最大步时间: 0.0007秒\n",
      "- 最小步时间: 0.0001秒\n",
      "- 平均奖励: 0.0624\n",
      "\n",
      "🚀 性能优秀: 适合大规模训练\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 性能测试\n",
    "def benchmark_environment(num_steps=100):\n",
    "    env = create_cm_env(difficulty=\"normal\")\n",
    "    observations= env.reset()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_rewards = []\n",
    "    step_times = []\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        step_start = time.time()\n",
    "        \n",
    "        # 随机动作\n",
    "        actions = {agent_id: np.random.choice(env.get_avail_actions(agent_id)) \n",
    "                  for agent_id in env.agent_ids}\n",
    "        \n",
    "        # 执行步骤\n",
    "        observations, rewards, done, info = env.step(actions)\n",
    "        \n",
    "        step_end = time.time()\n",
    "        step_times.append(step_end - step_start)\n",
    "        total_rewards.append(np.mean(list(rewards.values())))\n",
    "        \n",
    "        # 如果回合结束，重置\n",
    "        for agent_id in done:\n",
    "            if done[agent_id]:\n",
    "                observations = env.reset()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return {\n",
    "        'total_time': total_time,\n",
    "        'avg_step_time': np.mean(step_times),\n",
    "        'steps_per_second': num_steps / total_time,\n",
    "        'avg_reward': np.mean(total_rewards),\n",
    "        'max_step_time': np.max(step_times),\n",
    "        'min_step_time': np.min(step_times)\n",
    "    }\n",
    "\n",
    "# 运行基准测试\n",
    "results = benchmark_environment(200)\n",
    "\n",
    "print(\"性能测试结果:\")\n",
    "print(f\"- 总时间: {results['total_time']:.3f}秒\")\n",
    "print(f\"- 平均每步时间: {results['avg_step_time']:.4f}秒\")\n",
    "print(f\"- 每秒步数: {results['steps_per_second']:.1f}\")\n",
    "print(f\"- 最大步时间: {results['max_step_time']:.4f}秒\")\n",
    "print(f\"- 最小步时间: {results['min_step_time']:.4f}秒\")\n",
    "print(f\"- 平均奖励: {results['avg_reward']:.4f}\")\n",
    "\n",
    "# 性能评估\n",
    "if results['avg_step_time'] < 0.01:\n",
    "    print(\"\\n🚀 性能优秀: 适合大规模训练\")\n",
    "elif results['avg_step_time'] < 0.05:\n",
    "    print(\"\\n✅ 性能良好: 适合常规训练\")\n",
    "elif results['avg_step_time'] < 0.1:\n",
    "    print(\"\\n⚠️  性能一般: 可以用于训练，但可能较慢\")\n",
    "else:\n",
    "    print(\"\\n❌ 性能较慢: 建议优化后再进行大规模训练\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高级功能\n",
    "\n",
    "演示环境的高级功能，包括动作掩码、详细观察分析等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "动作掩码演示:\n",
      "\n",
      "智能体 agent_0:\n",
      "  - 当前位置: (1, 2)\n",
      "  - 可用动作数量: 5\n",
      "  - 可用动作: [0, 1, 2, 3, 4]\n",
      "  - 可用动作名称: ['等待', '上移动', '下移动', '左移动', '右移动']\n",
      "\n",
      "智能体 agent_1:\n",
      "  - 当前位置: (4, 2)\n",
      "  - 可用动作数量: 5\n",
      "  - 可用动作: [0, 1, 2, 3, 4]\n",
      "  - 可用动作名称: ['等待', '上移动', '下移动', '左移动', '右移动']\n"
     ]
    }
   ],
   "source": [
    "# 动作掩码演示\n",
    "env = create_cm_env(difficulty=\"normal\")\n",
    "observations = env.reset()\n",
    "\n",
    "print(\"动作掩码演示:\")\n",
    "for agent_id in env.agent_ids:\n",
    "    avail_actions = env.get_avail_actions(agent_id)\n",
    "    print(f\"\\n智能体 {agent_id}:\")\n",
    "    print(f\"  - 当前位置: ({env.game_state.get_agent(agent_id).position.x}, {env.game_state.get_agent(agent_id).position.y})\")\n",
    "    print(f\"  - 可用动作数量: {len(avail_actions)}\")\n",
    "    print(f\"  - 可用动作: {avail_actions}\")\n",
    "    \n",
    "    # 动作名称映射\n",
    "    action_names = {\n",
    "        0: \"等待\", 1: \"上移动\", 2: \"下移动\", 3: \"左移动\", 4: \"右移动\"\n",
    "    }\n",
    "    \n",
    "    print(f\"  - 可用动作名称: {[action_names[a] for a in avail_actions]}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观察空间详细分析:\n",
      "\n",
      "智能体 agent_0 的观察分析:\n",
      "- 观察维度: (8,)\n",
      "- 数据类型: float32\n",
      "- 最小值: -0.1429\n",
      "- 最大值: 0.5714\n",
      "- 均值: 0.2857\n",
      "- 标准差: 0.2143\n",
      "\n",
      "观察组成部分分析:\n",
      "- 自身位置 (0-2): [0.2857143 0.5714286]\n",
      "- 箱子中心 (2-4): [0.35714287 0.35714287]\n",
      "- 目标中心 (4-6): [0.07142857 0.5       ]\n",
      "- 相对agent_1位置 (6-8): [ 0.2857143  -0.14285715]\n"
     ]
    }
   ],
   "source": [
    "# 观察空间详细分析\n",
    "env = create_cm_env(difficulty=\"normal\")\n",
    "observations = env.reset()\n",
    "\n",
    "print(\"观察空间详细分析:\")\n",
    "\n",
    "# 分析一个智能体的观察\n",
    "sample_agent_id = env.agent_ids[0]\n",
    "obs = observations[sample_agent_id]\n",
    "\n",
    "print(f\"\\n智能体 {sample_agent_id} 的观察分析:\")\n",
    "print(f\"- 观察维度: {obs.shape}\")\n",
    "print(f\"- 数据类型: {obs.dtype}\")\n",
    "print(f\"- 最小值: {obs.min():.4f}\")\n",
    "print(f\"- 最大值: {obs.max():.4f}\")\n",
    "print(f\"- 均值: {obs.mean():.4f}\")\n",
    "print(f\"- 标准差: {obs.std():.4f}\")\n",
    "\n",
    "# 按观察组成部分分析\n",
    "print(\"\\n观察组成部分分析:\")\n",
    "print(f\"- 自身位置 (0-2): {obs[0:2]}\")\n",
    "print(f\"- 箱子中心 (2-4): {obs[2:4]}\")\n",
    "print(f\"- 目标中心 (4-6): {obs[4:6]}\")\n",
    "\n",
    "if env.n_agents > 1:\n",
    "    other_agent_start = 6\n",
    "    for i, other_id in enumerate(env.agent_ids):\n",
    "        if other_id != sample_agent_id:\n",
    "            rel_pos = obs[other_agent_start:other_agent_start+2]\n",
    "            print(f\"- 相对{other_id}位置 ({other_agent_start}-{other_agent_start+2}): {rel_pos}\")\n",
    "            other_agent_start += 2\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推动机制分析:\n",
      "\n",
      "箱子信息:\n",
      "- 位置: (2, 2)\n",
      "- 大小: 2x2\n",
      "- 占据位置: [(2, 2), (2, 3), (3, 2), (3, 3)]\n",
      "\n",
      "智能体推动能力分析:\n",
      "- agent_0:\n",
      "  - 位置: (2, 4)\n",
      "  - 可推动: True\n",
      "  - 推动侧: right\n",
      "- agent_1:\n",
      "  - 位置: (1, 2)\n",
      "  - 可推动: True\n",
      "  - 推动侧: top\n",
      "\n",
      "当前推动状态:\n",
      "- 推动智能体: ['agent_0', 'agent_1']\n",
      "- 推动侧: ['top', 'right']\n",
      "- 推动方向: [-1  1]\n"
     ]
    }
   ],
   "source": [
    "# 推动机制分析\n",
    "env = create_cm_env(difficulty=\"easy\")\n",
    "observations = env.reset()\n",
    "\n",
    "print(\"推动机制分析:\")\n",
    "\n",
    "# 分析初始推动条件\n",
    "box = env.game_state.box\n",
    "print(f\"\\n箱子信息:\")\n",
    "print(f\"- 位置: ({box.position.x}, {box.position.y})\")\n",
    "print(f\"- 大小: {box.size}x{box.size}\")\n",
    "print(f\"- 占据位置: {[(p.x, p.y) for p in box.get_occupied_positions()]}\")\n",
    "\n",
    "print(f\"\\n智能体推动能力分析:\")\n",
    "for agent_id, agent in env.game_state.agents.items():\n",
    "    is_pushing = agent.is_pushing_box(box)\n",
    "    push_side = agent.get_push_side(box)\n",
    "    \n",
    "    print(f\"- {agent_id}:\")\n",
    "    print(f\"  - 位置: ({agent.position.x}, {agent.position.y})\")\n",
    "    print(f\"  - 可推动: {is_pushing}\")\n",
    "    print(f\"  - 推动侧: {push_side}\")\n",
    "\n",
    "pushing_agents = env.game_state.get_pushing_agents()\n",
    "push_sides = env.game_state.get_push_sides()\n",
    "\n",
    "print(f\"\\n当前推动状态:\")\n",
    "print(f\"- 推动智能体: {pushing_agents}\")\n",
    "print(f\"- 推动侧: {push_sides}\")\n",
    "print(f\"- 推动方向: {env.game_state.calculate_push_direction(push_sides)}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "奖励系统分析:\n",
      "回合 1: 总奖励 = 2.460, 成功 = {'agent_0': False, 'agent_1': False}, 距离 = 4.47\n",
      "回合 2: 总奖励 = 4.431, 成功 = {'agent_0': False, 'agent_1': False}, 距离 = 3.16\n",
      "回合 3: 总奖励 = 2.639, 成功 = {'agent_0': False, 'agent_1': False}, 距离 = 1.41\n",
      "回合 4: 总奖励 = 1.671, 成功 = {'agent_0': False, 'agent_1': False}, 距离 = 1.00\n",
      "回合 5: 总奖励 = 1.323, 成功 = {'agent_0': False, 'agent_1': False}, 距离 = 3.00\n",
      "回合 6: 总奖励 = 1.557, 成功 = {'agent_0': False, 'agent_1': False}, 距离 = 3.00\n",
      "回合 7: 总奖励 = -0.483, 成功 = {'agent_0': False, 'agent_1': False}, 距离 = 2.83\n",
      "回合 8: 总奖励 = 1.003, 成功 = {'agent_0': False, 'agent_1': False}, 距离 = 1.41\n",
      "回合 9: 总奖励 = -0.624, 成功 = {'agent_0': False, 'agent_1': False}, 距离 = 2.24\n",
      "回合 10: 总奖励 = 4.266, 成功 = {'agent_0': False, 'agent_1': False}, 距离 = 2.00\n",
      "\n",
      "奖励统计分析:\n",
      "\n",
      "agent_0:\n",
      "  - 平均奖励: 0.0912\n",
      "  - 标准差: 0.2167\n",
      "  - 最大奖励: 0.4850\n",
      "  - 最小奖励: -0.1250\n",
      "  - 正奖励比例: 24.00%\n",
      "\n",
      "agent_1:\n",
      "  - 平均奖励: 0.0912\n",
      "  - 标准差: 0.2167\n",
      "  - 最大奖励: 0.4850\n",
      "  - 最小奖励: -0.1250\n",
      "  - 正奖励比例: 24.00%\n",
      "\n",
      "团队统计:\n",
      "- 平均回合奖励: 1.824 ± 1.617\n",
      "- 成功回合数: 0/10 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# 奖励系统分析\n",
    "env = create_cm_env(difficulty=\"normal\")\n",
    "\n",
    "print(\"奖励系统分析:\")\n",
    "\n",
    "# 执行多个步骤收集奖励数据\n",
    "all_rewards = {agent_id: [] for agent_id in env.agent_ids}\n",
    "episode_rewards = []\n",
    "successful_episodes = 0\n",
    "\n",
    "for episode in range(10):\n",
    "    observations = env.reset()\n",
    "    episode_total = 0\n",
    "    \n",
    "    for step in range(10):\n",
    "        actions = {agent_id: np.random.choice(env.get_avail_actions(agent_id)) \n",
    "                  for agent_id in env.agent_ids}\n",
    "        observations, rewards, done, info = env.step(actions)\n",
    "        \n",
    "        for agent_id, reward in rewards.items():\n",
    "            all_rewards[agent_id].append(reward)\n",
    "            episode_total += reward\n",
    "        \n",
    "        for agent_id, done_ in done.items():\n",
    "            if done_:\n",
    "                successful_episodes += 1\n",
    "    \n",
    "    episode_rewards.append(episode_total)\n",
    "    print(f\"回合 {episode + 1}: 总奖励 = {episode_total:.3f}, 成功 = {done}, 距离 = {info['distance_to_goal']:.2f}\")\n",
    "\n",
    "print(\"\\n奖励统计分析:\")\n",
    "for agent_id, rewards in all_rewards.items():\n",
    "    if rewards:\n",
    "        print(f\"\\n{agent_id}:\")\n",
    "        print(f\"  - 平均奖励: {np.mean(rewards):.4f}\")\n",
    "        print(f\"  - 标准差: {np.std(rewards):.4f}\")\n",
    "        print(f\"  - 最大奖励: {np.max(rewards):.4f}\")\n",
    "        print(f\"  - 最小奖励: {np.min(rewards):.4f}\")\n",
    "        print(f\"  - 正奖励比例: {np.mean(np.array(rewards) > 0):.2%}\")\n",
    "\n",
    "print(f\"\\n团队统计:\")\n",
    "print(f\"- 平均回合奖励: {np.mean(episode_rewards):.3f} ± {np.std(episode_rewards):.3f}\")\n",
    "print(f\"- 成功回合数: {successful_episodes}/10 ({successful_episodes/10:.1%})\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本教程介绍了CM环境的主要功能和使用方法：\n",
    "\n",
    "1. **基础环境创建和使用**：展示了如何创建环境、重置、执行步骤\n",
    "2. **CTDE兼容性**：演示了集中式训练分布式执行的支持\n",
    "3. **可视化功能**：展示了静态和动态可视化能力\n",
    "4. **配置管理**：介绍了不同难度和专用配置\n",
    "5. **性能测试**：评估了环境执行效率\n",
    "6. **高级功能**：演示了动作掩码、观察分析等功能\n",
    "7. **完整示例**：提供了简单智能体的实现\n",
    "\n",
    "### 关键特点：\n",
    "- ✅ **协作需求**：多个智能体必须协作才能成功推动箱子\n",
    "- ✅ **简单直观**：低维观测和动作空间，易于理解和学习\n",
    "- ✅ **CTDE兼容**：支持主流多智能体强化学习算法\n",
    "- ✅ **高性能**：单步执行时间<0.01秒，适合大规模训练\n",
    "- ✅ **可视化**：丰富的可视化功能便于分析和调试\n",
    "- ✅ **可配置**：多种预设和自定义配置支持\n",
    "- ✅ **概率机制**：推动成功概率机制增加挑战性\n",
    "\n",
    "### 环境优势：\n",
    "1. **算法测试友好**：简单的状态空间便于算法快速收敛\n",
    "2. **协作机制明确**：智能体必须协作才能获得高奖励\n",
    "3. **难度可调**：从简单的2智能体到复杂的4智能体配置\n",
    "4. **评估直观**：任务成功与否一目了然\n",
    "5. **研究价值高**：适合测试协作算法的各个方面\n",
    "\n",
    "CM环境现在可以用于多智能体强化学习的研究和实验了！它特别适合：\n",
    "- 测试新的协作算法\n",
    "- 验证现有算法的协作能力\n",
    "- 教学和演示多智能体系统\n",
    "- 算法性能基准测试"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
