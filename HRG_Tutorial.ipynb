{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRG (Heterogeneous Resource Gathering) 环境使用教程\n",
    "\n",
    "本教程将介绍如何使用HRG环境进行多智能体强化学习实验。\n",
    "\n",
    "## 目录\n",
    "1. [环境简介](#环境简介)\n",
    "2. [基础使用](#基础使用)\n",
    "3. [CTDE兼容性](#ctde兼容性)\n",
    "4. [可视化功能](#可视化功能)\n",
    "5. [配置管理](#配置管理)\n",
    "6. [性能测试](#性能测试)\n",
    "7. [高级功能](#高级功能)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境简介\n",
    "\n",
    "HRG是一个异构资源收集环境，包含三种不同角色的智能体：\n",
    "- **侦察兵(Scout)**：视野大，移动快，负责探索\n",
    "- **工人(Worker)**：可以采集资源，负责生产\n",
    "- **运输车(Transporter)**：容量大，负责物流\n",
    "\n",
    "环境的目标是团队协作最大化资源收集效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRG环境详解：观测空间、动作空间与奖励机制\n",
    "\n",
    "## 1. 观测空间（80维）详细解析\n",
    "\n",
    "观测空间定义在 `env_hrg.py` 的 `_get_observation` 方法中，共80维，结构如下：\n",
    "\n",
    "### 1.1 自身状态（10维）\n",
    "\n",
    "| 维度 | 特征 | 描述 |\n",
    "|------|------|------|\n",
    "| 2 | 位置信息 | `position.x/grid_size`, `position.y/grid_size` - 归一化的坐标 |\n",
    "| 3 | 角色类型 | one-hot编码 `[侦察兵, 工人, 运输车]` |\n",
    "| 2 | 库存状态 | `gold/10.0`, `wood/10.0` - 归一化的资源数量 |\n",
    "| 2 | 能量和冷却 | `energy/100.0`, `action_cooldown/2.0` - 能量和动作冷却时间 |\n",
    "| 1 | 基地距离 | 到基地的曼哈顿距离归一化值 |\n",
    "\n",
    "### 1.2 时间信息（1维）\n",
    "\n",
    "- **剩余时间比例**：`1.0 - (current_step / max_steps)`\n",
    "- 提供任务进度信息，帮助智能体规划长期策略\n",
    "\n",
    "### 1.3 可见实体信息（最多50维）\n",
    "\n",
    "最多观察10个实体，每个实体5维信息：\n",
    "\n",
    "| 维度 | 特征 | 描述 |\n",
    "|------|------|------|\n",
    "| 2 | 相对位置 | `(pos.x - agent.x)/vision_range`, `(pos.y - agent.y)/vision_range` |\n",
    "| 3 | 实体类型 | \n",
    "  - **其他智能体**：3维one-hot编码 `[侦察兵, 工人, 运输车]`\n",
    "  - **资源**：2维one-hot编码 `[金矿, 木材]` + 1维资源数量 |\n",
    "\n",
    "**关键特性**：\n",
    "- 不同角色视野范围不同：侦察兵(5格) > 运输车(4格) > 工人(3格)\n",
    "- 体现能力异构性，鼓励角色专业化\n",
    "\n",
    "### 1.4 通信历史（10维）\n",
    "\n",
    "最近3条消息的简化编码（基于发送者类型）：\n",
    "- **侦察兵消息**：0.3\n",
    "- **工人消息**：0.6  \n",
    "- **运输车消息**：0.9\n",
    "\n",
    "**设计哲学**：\n",
    "- 模拟真实通信中的\"信道质量\"\n",
    "- 避免直接传递完整语义，增加学习难度\n",
    "- 鼓励智能体从有限信息中推断意图\n",
    "\n",
    "## 2. 动作空间（8维）详细解析\n",
    "\n",
    "动作空间定义在 `core.py` 中，包含8个离散动作：\n",
    "\n",
    "### 2.1 移动动作（4个）\n",
    "\n",
    "| 动作ID | 名称 | 描述 |\n",
    "|--------|------|------|\n",
    "| 0 | MOVE_NORTH | 向北移动 |\n",
    "| 1 | MOVE_SOUTH | 向南移动 |\n",
    "| 2 | MOVE_WEST | 向西移动 |\n",
    "| 3 | MOVE_EAST | 向东移动 |\n",
    "\n",
    "**物理约束**：\n",
    "- 侦察兵：速度2.0（可连续移动两次）\n",
    "- 工人：速度1.0\n",
    "- 运输车：速度1.5（可移动1格+0.5概率移动第2格）\n",
    "\n",
    "### 2.2 交互动作（4个）\n",
    "\n",
    "| 动作ID | 名称 | 描述 |\n",
    "|--------|------|------|\n",
    "| 4 | GATHER | 采集当前位置的资源<br>• 工人专属动作，侦察兵无法采集<br>• 需要连续执行多个回合才能完成采集<br>• 采集时间：金矿4回合，木材2回合 |\n",
    "| 5 | TRANSFER | 与相邻智能体转移资源<br>• 工人→运输车：转移采集到的资源<br>• 运输车→工人：通常为空操作<br>• 只有在相邻位置且有对应类型的智能体时才能执行 |\n",
    "| 6 | DEPOSIT | 在基地存放资源<br>• 运输车专属动作，只有运输车可以存放资源<br>• 必须在基地位置(0,0)才能执行<br>• 获得资源价值的50%作为奖励 |\n",
    "| 7 | WAIT | 等待/跳过回合 |\n",
    "\n",
    "### 2.3 角色限制\n",
    "\n",
    "不同角色有不同的动作限制：\n",
    "\n",
    "| 动作 | 侦察兵 | 工人 | 运输车 |\n",
    "|------|--------|------|--------|\n",
    "| GATHER | ❌ | ✅ | ❌ |\n",
    "| DEPOSIT | ❌ | ❌ | ✅ |\n",
    "| TRANSFER | ✅ | ✅ | ✅ |\n",
    "| MOVE | ✅ | ✅ | ✅ |\n",
    "\n",
    "**设计优势**：\n",
    "- 强制角色分化，避免所有智能体选择相同策略\n",
    "- 模拟现实世界中的专业分工\n",
    "- 促进团队协作需求\n",
    "\n",
    "## 3. 奖励机制详细解析\n",
    "\n",
    "奖励系统设计在 `env_hrg.py` 的 `_calculate_team_reward` 方法中，包含个人奖励和团队奖励。\n",
    "\n",
    "### 3.1 个人动作奖励\n",
    "\n",
    "#### 1. 移动奖励（_move_agent）\n",
    "- **成功移动**：0.0（无奖励，仅消耗能量）\n",
    "- **无效移动**：-0.1（撞墙或移动到无效位置）\n",
    "- **能量消耗**：根据角色类型消耗不同能量\n",
    "  - 侦察兵：0.05/步\n",
    "  - 工人：0.02/步\n",
    "  - 运输车：0.03/步\n",
    "\n",
    "#### 2. 采集奖励（_gather_resource）\n",
    "- **采集完成**：获得资源价值的10%\n",
    "  - 金矿：10.0 × 0.1 = 1.0\n",
    "  - 木材：2.0 × 0.1 = 0.2\n",
    "- **采集过程中**：0.0（需要多回合完成）\n",
    "- **能量消耗**：工人采集时消耗0.08能量\n",
    "\n",
    "#### 3. 转移奖励（_transfer_resource）\n",
    "- **成功转移**：获得转移资源价值的5%\n",
    "  - 金矿：10.0 × 0.05 = 0.5\n",
    "  - 木材：2.0 × 0.05 = 0.1\n",
    "- **能量消耗**：转移时消耗0.1能量\n",
    "\n",
    "#### 4. 存放奖励（_deposit_resources）\n",
    "- **成功存放**：获得资源价值的50%\n",
    "  - 金矿：10.0 × 0.5 = 5.0\n",
    "  - 木材：2.0 × 0.5 = 1.0\n",
    "- 这是最大的单次奖励，鼓励资源成功运回基地\n",
    "\n",
    "### 3.2 团队奖励（_calculate_team_reward）\n",
    "\n",
    "| 奖励类型 | 值 | 描述 |\n",
    "|---------|-----|------|\n",
    "| 时间惩罚 | -0.01/步 | 鼓励快速完成任务 |\n",
    "| 资源多样性奖励 | +0.1 (金矿), +0.05 (木材) | 第一次存放特定资源时给予额外奖励 |\n",
    "| 协作奖励 | 动态计算 | 基于团队整体效率的附加奖励 |\n",
    "\n",
    "### 3.3 角色能力差异\n",
    "\n",
    "| 角色 | 优势 | 限制 | 能量消耗 |\n",
    "|------|------|------|----------|\n",
    "| **侦察兵** | 视野范围5，移动速度2.0 | 无法采集，携带容量0 | 移动0.05 |\n",
    "| **工人** | 可采集，携带容量2 | 视野范围3，移动速度1.0 | 移动0.02，采集0.08 |\n",
    "| **运输车** | 可存放资源，携带容量5，移动速度1.5 | 无法采集，视野范围4 | 移动0.03，转移0.1 |\n",
    "\n",
    "## 4. 奖励设计理念\n",
    "\n",
    "### 4.1 核心原则\n",
    "\n",
    "1. **鼓励协作**\n",
    "   - 转移奖励(0.5) < 存放奖励(5.0)，形成\"采集→转移→存放\"的价值链\n",
    "   - 团队共享最终奖励，强化集体目标\n",
    "\n",
    "2. **效率导向**\n",
    "   - 每步-0.01的时间惩罚，促使智能体快速行动\n",
    "   - 能量消耗机制防止无限循环行为\n",
    "\n",
    "3. **角色专精**\n",
    "   - 不同角色有不同的最优策略\n",
    "   - 侦察兵应专注于探索和通信\n",
    "   - 工人应专注于资源采集\n",
    "   - 运输车应专注于物流调度\n",
    "\n",
    "4. **目标明确**\n",
    "   - 最终目标是收集并存放资源到基地\n",
    "   - 奖励信号清晰指向核心任务\n",
    "\n",
    "### 4.2 多层次奖励结构\n",
    "\n",
    "```\n",
    "+-----------------------+\n",
    "|      最终目标         |\n",
    "|  资源存放至基地       |\n",
    "+-----------------------+\n",
    "           |\n",
    "           v\n",
    "+-----------------------+\n",
    "|     中间目标          |\n",
    "|  资源采集与转移       |\n",
    "+-----------------------+\n",
    "           |\n",
    "           v\n",
    "+-----------------------+\n",
    "|     基础行为          |\n",
    "|  探索、移动、避障     |\n",
    "+-----------------------+\n",
    "```\n",
    "\n",
    "## 5. 总结\n",
    "\n",
    "HRG环境是一个典型的异构多智能体协作环境，具有以下特点：\n",
    "\n",
    "- **80维观测空间**：提供了丰富的环境信息，包括自身状态、时间信息、可见实体和通信历史\n",
    "- **8维动作空间**：支持复杂的决策，包含移动、采集、转移和存放等交互动作\n",
    "- **精心设计的奖励机制**：通过多层次奖励引导智能体学习高效协作策略\n",
    "\n",
    "**环境优势**：\n",
    "1. **角色分化需求高**：强制不同角色承担不同职责\n",
    "2. **通信必要性强**：侦察兵发现资源后必须通知工人\n",
    "3. **物流链条完整**：从探索到采集再到运输形成完整闭环\n",
    "4. **挑战性适中**：既不过于简单也不过于复杂，适合算法测试\n",
    "\n",
    "这个环境为测试多智能体协作算法提供了一个理想的平台，特别是对于验证RACGA等强调角色动态性和双通道通信的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HRG环境导入成功！\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加环境路径\n",
    "import sys\n",
    "sys.path.append(\"..\")  # 将上级目录添加到系统路径\n",
    "\n",
    "# 导入HRG环境\n",
    "from Env.HRG import create_hrg_env, create_hrg_ctde_env\n",
    "from Env.HRG.config import get_config_by_name\n",
    "from Env.HRG.renderer import MatplotlibRenderer\n",
    "\n",
    "print(\"HRG环境导入成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境信息:\n",
      "- 智能体数量: 6\n",
      "- 智能体ID: ['scout_0', 'scout_1', 'transporter_0', 'worker_0', 'worker_1', 'worker_2']\n",
      "- 网格大小: 10x10\n",
      "- 最大步数: 200\n",
      "- 观察空间维度: 80\n",
      "- 动作空间维度: 8\n"
     ]
    }
   ],
   "source": [
    "# 创建基础HRG环境\n",
    "env = create_hrg_env(difficulty=\"normal\", render_mode=\"\")\n",
    "\n",
    "print(f\"环境信息:\")\n",
    "print(f\"- 智能体数量: {env.n_agents}\")\n",
    "print(f\"- 智能体ID: {env.agent_ids}\")\n",
    "print(f\"- 网格大小: {env.config.grid_size}x{env.config.grid_size}\")\n",
    "print(f\"- 最大步数: {env.config.max_steps}\")\n",
    "print(f\"- 观察空间维度: {list(env.obs_dims.values())[0]}\")\n",
    "print(f\"- 动作空间维度: {list(env.act_dims.values())[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始观察信息:\n",
      "- scout_0: 观察形状=(80,), 数据类型=float32\n",
      "  最小值=-0.200, 最大值=1.000\n"
     ]
    }
   ],
   "source": [
    "# 重置环境\n",
    "observations = env.reset()\n",
    "\n",
    "print(\"初始观察信息:\")\n",
    "for agent_id, obs in observations.items():\n",
    "    print(f\"- {agent_id}: 观察形状={obs.shape}, 数据类型={obs.dtype}\")\n",
    "    print(f\"  最小值={obs.min():.3f}, 最大值={obs.max():.3f}\")\n",
    "    break  # 只显示第一个智能体的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始执行回合...\n",
      "步数 10: 平均奖励 = -0.010\n",
      "步数 20: 平均奖励 = -0.060\n",
      "步数 30: 平均奖励 = -0.010\n",
      "步数 40: 平均奖励 = -0.027\n",
      "步数 50: 平均奖励 = -0.060\n",
      "\n",
      "回合结果:\n",
      "- scout_0: 总奖励 = -1.700\n",
      "- scout_1: 总奖励 = -3.000\n",
      "- transporter_0: 总奖励 = -1.200\n",
      "- worker_0: 总奖励 = -1.900\n",
      "- worker_1: 总奖励 = -1.100\n",
      "- worker_2: 总奖励 = -1.000\n",
      "- 团队平均奖励: -1.650\n",
      "- 环境总得分: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 执行一个简单的回合\n",
    "total_rewards = {agent_id: 0 for agent_id in env.agent_ids}\n",
    "step_count = 0\n",
    "\n",
    "print(\"开始执行回合...\")\n",
    "\n",
    "while step_count < 50:  # 限制步数用于演示\n",
    "    # 随机选择动作\n",
    "    actions = {}\n",
    "    for agent_id in env.agent_ids:\n",
    "        # 获取可用动作\n",
    "        avail_actions = env.get_avail_actions(agent_id)\n",
    "        action = np.random.choice(avail_actions)\n",
    "        actions[agent_id] = action\n",
    "    \n",
    "    # 执行动作\n",
    "    observations, rewards, dones, infos = env.step(actions)\n",
    "    \n",
    "    # 累积奖励\n",
    "    for agent_id, reward in rewards.items():\n",
    "        total_rewards[agent_id] += reward\n",
    "    \n",
    "    step_count += 1\n",
    "    \n",
    "    # 打印进度\n",
    "    if step_count % 10 == 0:\n",
    "        avg_reward = np.mean(list(rewards.values()))\n",
    "        print(f\"步数 {step_count}: 平均奖励 = {avg_reward:.3f}\")\n",
    "    \n",
    "    # 检查是否结束\n",
    "    if any(dones.values()):\n",
    "        print(f\"回合在步数 {step_count} 结束\")\n",
    "        break\n",
    "\n",
    "print(\"\\n回合结果:\")\n",
    "for agent_id, total_reward in total_rewards.items():\n",
    "    print(f\"- {agent_id}: 总奖励 = {total_reward:.3f}\")\n",
    "print(f\"- 团队平均奖励: {np.mean(list(total_rewards.values())):.3f}\")\n",
    "print(f\"- 环境总得分: {env.game_state.total_score:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境已关闭\n"
     ]
    }
   ],
   "source": [
    "# 关闭环境\n",
    "env.close()\n",
    "print(\"环境已关闭\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTDE兼容性\n",
    "\n",
    "HRG环境提供了CTDE（集中式训练分布式执行）包装器，支持QMIX、VDN等算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试全局状态类型: concat\n",
      "  - 全局状态维度: (480,)\n",
      "  - 非零元素数量: 72\n",
      "  - 数据范围: [-0.200, 1.000]\n",
      "  - Info中全局状态形状: (480,)\n",
      "  - 状态一致性: False\n",
      "\n",
      "测试全局状态类型: mean\n",
      "  - 全局状态维度: (80,)\n",
      "  - 非零元素数量: 14\n",
      "  - 数据范围: [-0.247, 1.000]\n",
      "  - Info中全局状态形状: (80,)\n",
      "  - 状态一致性: False\n",
      "\n",
      "测试全局状态类型: max\n",
      "  - 全局状态维度: (80,)\n",
      "  - 非零元素数量: 14\n",
      "  - 数据范围: [0.000, 1.000]\n",
      "  - Info中全局状态形状: (80,)\n",
      "  - 状态一致性: False\n",
      "\n",
      "测试全局状态类型: attention\n",
      "  - 全局状态维度: (86,)\n",
      "  - 非零元素数量: 21\n",
      "  - 数据范围: [-0.171, 1.000]\n",
      "  - Info中全局状态形状: (86,)\n",
      "  - 状态一致性: False\n"
     ]
    }
   ],
   "source": [
    "# 测试不同类型的全局状态表示\n",
    "global_state_types = [\"concat\", \"mean\", \"max\", \"attention\"]\n",
    "\n",
    "for state_type in global_state_types:\n",
    "    print(f\"\\n测试全局状态类型: {state_type}\")\n",
    "    \n",
    "    # 创建CTDE环境\n",
    "    ctde_env = create_hrg_ctde_env(\n",
    "        \"normal_ctde\",\n",
    "        global_state_type=state_type\n",
    "    )\n",
    "    \n",
    "    # 重置环境\n",
    "    observations = ctde_env.reset()\n",
    "    \n",
    "    # 获取全局状态\n",
    "    global_state = ctde_env.get_global_state()\n",
    "    \n",
    "    print(f\"  - 全局状态维度: {global_state.shape}\")\n",
    "    print(f\"  - 非零元素数量: {np.count_nonzero(global_state)}\")\n",
    "    print(f\"  - 数据范围: [{global_state.min():.3f}, {global_state.max():.3f}]\")\n",
    "    \n",
    "    # 执行一步并检查info中的全局状态\n",
    "    actions = {agent_id: 7 for agent_id in ctde_env.agent_ids}\n",
    "    obs, rewards, dones, infos = ctde_env.step(actions)\n",
    "    \n",
    "    if 'global_state' in infos:\n",
    "        info_state = infos['global_state']\n",
    "        print(f\"  - Info中全局状态形状: {info_state.shape}\")\n",
    "        print(f\"  - 状态一致性: {np.allclose(global_state, info_state)}\")\n",
    "    \n",
    "    ctde_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTDE环境信息:\n",
      "- env_name: HRG-CTDE\n",
      "- n_agents: 6\n",
      "- agent_ids: ['scout_0', 'scout_1', 'transporter_0', 'worker_0', 'worker_1', 'worker_2']\n",
      "- obs_dims: {'scout_0': 80, 'scout_1': 80, 'transporter_0': 80, 'worker_0': 80, 'worker_1': 80, 'worker_2': 80}\n",
      "- act_dims: {'scout_0': 8, 'scout_1': 8, 'transporter_0': 8, 'worker_0': 8, 'worker_1': 8, 'worker_2': 8}\n",
      "- global_state_dim: 480\n",
      "- episode_limit: 200\n",
      "- global_state_type: concat\n"
     ]
    }
   ],
   "source": [
    "# 获取CTDE环境信息\n",
    "ctde_env = create_hrg_ctde_env(\"normal_ctde\")\n",
    "env_info = ctde_env.get_env_info()\n",
    "\n",
    "print(\"CTDE环境信息:\")\n",
    "for key, value in env_info.items():\n",
    "    print(f\"- {key}: {value}\")\n",
    "\n",
    "ctde_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置管理\n",
    "\n",
    "HRG环境提供了丰富的配置选项，支持不同的实验设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "难度配置: easy\n",
      "  - 网格大小: 8x8\n",
      "  - 最大步数: 300\n",
      "  - 障碍物数量: 0\n",
      "  - 金矿数量: 2\n",
      "  - 木材数量: 15\n",
      "\n",
      "难度配置: normal\n",
      "  - 网格大小: 10x10\n",
      "  - 最大步数: 200\n",
      "  - 障碍物数量: 10\n",
      "  - 金矿数量: 3\n",
      "  - 木材数量: 10\n",
      "\n",
      "难度配置: hard\n",
      "  - 网格大小: 12x12\n",
      "  - 最大步数: 150\n",
      "  - 障碍物数量: 20\n",
      "  - 金矿数量: 4\n",
      "  - 木材数量: 8\n"
     ]
    }
   ],
   "source": [
    "# 测试不同难度配置\n",
    "difficulties = [\"easy\", \"normal\", \"hard\"]\n",
    "\n",
    "for difficulty in difficulties:\n",
    "    print(f\"\\n难度配置: {difficulty}\")\n",
    "    \n",
    "    env = create_hrg_env(difficulty=difficulty)\n",
    "    \n",
    "    print(f\"  - 网格大小: {env.config.grid_size}x{env.config.grid_size}\")\n",
    "    print(f\"  - 最大步数: {env.config.max_steps}\")\n",
    "    print(f\"  - 障碍物数量: {env.config.num_obstacles}\")\n",
    "    print(f\"  - 金矿数量: {env.config.num_gold}\")\n",
    "    print(f\"  - 木材数量: {env.config.num_wood}\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "专用配置: communication\n",
      "  - 配置描述: communication\n",
      "  - 网格: 10x10\n",
      "  - 步数: 250\n",
      "  - 障碍物: 15\n",
      "\n",
      "专用配置: coordination\n",
      "  - 配置描述: coordination\n",
      "  - 网格: 10x10\n",
      "  - 步数: 200\n",
      "  - 障碍物: 8\n",
      "\n",
      "专用配置: exploration\n",
      "  - 配置描述: exploration\n",
      "  - 网格: 15x15\n",
      "  - 步数: 400\n",
      "  - 障碍物: 25\n",
      "\n",
      "专用配置: role_test\n",
      "  - 配置描述: role_test\n",
      "  - 网格: 10x10\n",
      "  - 步数: 200\n",
      "  - 障碍物: 12\n"
     ]
    }
   ],
   "source": [
    "# 测试专用配置\n",
    "special_configs = [\"communication\", \"coordination\", \"exploration\", \"role_test\"]\n",
    "\n",
    "for config_name in special_configs:\n",
    "    print(f\"\\n专用配置: {config_name}\")\n",
    "    \n",
    "    try:\n",
    "        config = get_config_by_name(config_name)\n",
    "        env = create_hrg_env(difficulty=\"normal\")\n",
    "        \n",
    "        # 更新配置\n",
    "        env.config.grid_size = config.grid_size\n",
    "        env.config.max_steps = config.max_steps\n",
    "        env.config.num_obstacles = config.num_obstacles\n",
    "        \n",
    "        print(f\"  - 配置描述: {config_name}\")\n",
    "        print(f\"  - 网格: {config.grid_size}x{config.grid_size}\")\n",
    "        print(f\"  - 步数: {config.max_steps}\")\n",
    "        print(f\"  - 障碍物: {config.num_obstacles}\")\n",
    "        \n",
    "        env.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  - 错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自定义配置:\n",
      "- 网格大小: 8\n",
      "- 最大步数: 100\n",
      "- 障碍物: 5\n",
      "- 金矿: 2\n",
      "- 木材: 8\n",
      "\n",
      "自定义环境创建成功！\n"
     ]
    }
   ],
   "source": [
    "# 自定义配置示例\n",
    "from Env.HRG.env_hrg import HRGConfig\n",
    "\n",
    "# 创建自定义配置\n",
    "custom_config = HRGConfig(\n",
    "    grid_size=8,\n",
    "    max_steps=100,\n",
    "    num_obstacles=5,\n",
    "    num_gold=2,\n",
    "    num_wood=8,\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "print(\"自定义配置:\")\n",
    "print(f\"- 网格大小: {custom_config.grid_size}\")\n",
    "print(f\"- 最大步数: {custom_config.max_steps}\")\n",
    "print(f\"- 障碍物: {custom_config.num_obstacles}\")\n",
    "print(f\"- 金矿: {custom_config.num_gold}\")\n",
    "print(f\"- 木材: {custom_config.num_wood}\")\n",
    "\n",
    "# 创建环境\n",
    "custom_env = create_hrg_env(difficulty=\"normal\")\n",
    "custom_env.config = custom_config\n",
    "\n",
    "print(f\"\\n自定义环境创建成功！\")\n",
    "custom_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性能测试\n",
    "\n",
    "评估环境的执行性能，确保适合强化学习训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "性能测试结果:\n",
      "- 总时间: 0.172秒\n",
      "- 平均每步时间: 0.0008秒\n",
      "- 每秒步数: 1160.7\n",
      "- 最大步时间: 0.0013秒\n",
      "- 最小步时间: 0.0007秒\n",
      "- 平均奖励: -0.0285\n",
      "\n",
      "🚀 性能优秀: 适合大规模训练\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 性能测试\n",
    "def benchmark_environment(num_steps=100):\n",
    "    env = create_hrg_env(difficulty=\"normal\", render_mode=\"\")\n",
    "    observations = env.reset()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_rewards = []\n",
    "    step_times = []\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        step_start = time.time()\n",
    "        \n",
    "        # 随机动作\n",
    "        actions = {agent_id: np.random.choice(env.get_avail_actions(agent_id)) \n",
    "                  for agent_id in env.agent_ids}\n",
    "        \n",
    "        # 执行步骤\n",
    "        observations, rewards, dones, infos = env.step(actions)\n",
    "        \n",
    "        step_end = time.time()\n",
    "        step_times.append(step_end - step_start)\n",
    "        total_rewards.append(np.mean(list(rewards.values())))\n",
    "        \n",
    "        # 如果回合结束，重置\n",
    "        if any(dones.values()):\n",
    "            observations = env.reset()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return {\n",
    "        'total_time': total_time,\n",
    "        'avg_step_time': np.mean(step_times),\n",
    "        'steps_per_second': num_steps / total_time,\n",
    "        'avg_reward': np.mean(total_rewards),\n",
    "        'max_step_time': np.max(step_times),\n",
    "        'min_step_time': np.min(step_times)\n",
    "    }\n",
    "\n",
    "# 运行基准测试\n",
    "results = benchmark_environment(200)\n",
    "\n",
    "print(\"性能测试结果:\")\n",
    "print(f\"- 总时间: {results['total_time']:.3f}秒\")\n",
    "print(f\"- 平均每步时间: {results['avg_step_time']:.4f}秒\")\n",
    "print(f\"- 每秒步数: {results['steps_per_second']:.1f}\")\n",
    "print(f\"- 最大步时间: {results['max_step_time']:.4f}秒\")\n",
    "print(f\"- 最小步时间: {results['min_step_time']:.4f}秒\")\n",
    "print(f\"- 平均奖励: {results['avg_reward']:.4f}\")\n",
    "\n",
    "# 性能评估\n",
    "if results['avg_step_time'] < 0.01:\n",
    "    print(\"\\n🚀 性能优秀: 适合大规模训练\")\n",
    "elif results['avg_step_time'] < 0.05:\n",
    "    print(\"\\n✅ 性能良好: 适合常规训练\")\n",
    "elif results['avg_step_time'] < 0.1:\n",
    "    print(\"\\n⚠️  性能一般: 可以用于训练，但可能较慢\")\n",
    "else:\n",
    "    print(\"\\n❌ 性能较慢: 建议优化后再进行大规模训练\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高级功能\n",
    "\n",
    "演示环境的高级功能，包括动作掩码、详细观察分析等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "动作掩码演示:\n",
      "\n",
      "智能体 scout_0:\n",
      "  - 可用动作数量: 6\n",
      "  - 可用动作: [0, 1, 2, 3, 5, 7]\n",
      "  - 可用动作名称: ['上移动', '下移动', '左移动', '右移动', '传输', '等待']\n",
      "\n",
      "智能体 scout_1:\n",
      "  - 可用动作数量: 6\n",
      "  - 可用动作: [0, 1, 2, 3, 5, 7]\n",
      "  - 可用动作名称: ['上移动', '下移动', '左移动', '右移动', '传输', '等待']\n",
      "\n",
      "智能体 transporter_0:\n",
      "  - 可用动作数量: 8\n",
      "  - 可用动作: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "  - 可用动作名称: ['上移动', '下移动', '左移动', '右移动', '采集', '传输', '存放', '等待']\n",
      "\n",
      "智能体 worker_0:\n",
      "  - 可用动作数量: 7\n",
      "  - 可用动作: [0, 1, 2, 3, 4, 5, 7]\n",
      "  - 可用动作名称: ['上移动', '下移动', '左移动', '右移动', '采集', '传输', '等待']\n",
      "\n",
      "智能体 worker_1:\n",
      "  - 可用动作数量: 7\n",
      "  - 可用动作: [0, 1, 2, 3, 4, 5, 7]\n",
      "  - 可用动作名称: ['上移动', '下移动', '左移动', '右移动', '采集', '传输', '等待']\n",
      "\n",
      "智能体 worker_2:\n",
      "  - 可用动作数量: 7\n",
      "  - 可用动作: [0, 1, 2, 3, 4, 5, 7]\n",
      "  - 可用动作名称: ['上移动', '下移动', '左移动', '右移动', '采集', '传输', '等待']\n",
      "\n",
      "智能体 scout_0:\n",
      "  - 可用动作数量: 6\n",
      "  - 可用动作: [0, 1, 2, 3, 5, 7]\n",
      "  - 可用动作名称: ['上移动', '下移动', '左移动', '右移动', '传输', '等待']\n",
      "\n",
      "智能体 scout_1:\n",
      "  - 可用动作数量: 6\n",
      "  - 可用动作: [0, 1, 2, 3, 5, 7]\n",
      "  - 可用动作名称: ['上移动', '下移动', '左移动', '右移动', '传输', '等待']\n",
      "\n",
      "智能体 worker_0:\n",
      "  - 可用动作数量: 7\n",
      "  - 可用动作: [0, 1, 2, 3, 4, 5, 7]\n",
      "  - 可用动作名称: ['上移动', '下移动', '左移动', '右移动', '采集', '传输', '等待']\n",
      "\n",
      "智能体 worker_1:\n",
      "  - 可用动作数量: 7\n",
      "  - 可用动作: [0, 1, 2, 3, 4, 5, 7]\n",
      "  - 可用动作名称: ['上移动', '下移动', '左移动', '右移动', '采集', '传输', '等待']\n",
      "\n",
      "智能体 worker_2:\n",
      "  - 可用动作数量: 7\n",
      "  - 可用动作: [0, 1, 2, 3, 4, 5, 7]\n",
      "  - 可用动作名称: ['上移动', '下移动', '左移动', '右移动', '采集', '传输', '等待']\n",
      "\n",
      "智能体 transporter_0:\n",
      "  - 可用动作数量: 8\n",
      "  - 可用动作: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "  - 可用动作名称: ['上移动', '下移动', '左移动', '右移动', '采集', '传输', '存放', '等待']\n"
     ]
    }
   ],
   "source": [
    "# 动作掩码演示\n",
    "env = create_hrg_env(difficulty=\"normal\", render_mode=\"\")\n",
    "observations = env.reset()\n",
    "\n",
    "print(\"动作掩码演示:\")\n",
    "for agent_id in env.agent_ids:\n",
    "    avail_actions = env.get_avail_actions(agent_id)\n",
    "    print(f\"\\n智能体 {agent_id}:\")\n",
    "    print(f\"  - 可用动作数量: {len(avail_actions)}\")\n",
    "    print(f\"  - 可用动作: {avail_actions}\")\n",
    "    \n",
    "    # 动作名称映射\n",
    "    action_names = {\n",
    "        0: \"上移动\", 1: \"下移动\", 2: \"左移动\", 3: \"右移动\",\n",
    "        4: \"采集\", 5: \"传输\", 6: \"存放\", 7: \"等待\"\n",
    "    }\n",
    "    \n",
    "    print(f\"  - 可用动作名称: {[action_names[a] for a in avail_actions]}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观察空间详细分析:\n",
      "\n",
      "智能体 scout_0 的观察分析:\n",
      "- 观察维度: (80,)\n",
      "- 数据类型: float32\n",
      "- 最小值: -0.4000\n",
      "- 最大值: 1.0000\n",
      "- 均值: 0.0812\n",
      "- 标准差: 0.2941\n",
      "- 零值比例: 83.75%\n",
      "\n",
      "观察组成部分分析:\n",
      "- 自身状态 (0-10): min=0.000, max=1.000\n",
      "- 视野内实体 (10-60): min=-0.400, max=1.000\n",
      "- 消息信息 (60-70): min=0.000, max=0.000\n",
      "- 其他信息 (70-80): min=0.000, max=0.000\n"
     ]
    }
   ],
   "source": [
    "# 观察空间详细分析\n",
    "env = create_hrg_env(difficulty=\"normal\")\n",
    "observations = env.reset()\n",
    "\n",
    "print(\"观察空间详细分析:\")\n",
    "\n",
    "# 分析一个智能体的观察\n",
    "sample_agent_id = env.agent_ids[0]\n",
    "obs = observations[sample_agent_id]\n",
    "\n",
    "print(f\"\\n智能体 {sample_agent_id} 的观察分析:\")\n",
    "print(f\"- 观察维度: {obs.shape}\")\n",
    "print(f\"- 数据类型: {obs.dtype}\")\n",
    "print(f\"- 最小值: {obs.min():.4f}\")\n",
    "print(f\"- 最大值: {obs.max():.4f}\")\n",
    "print(f\"- 均值: {obs.mean():.4f}\")\n",
    "print(f\"- 标准差: {obs.std():.4f}\")\n",
    "print(f\"- 零值比例: {np.mean(obs == 0):.2%}\")\n",
    "\n",
    "# 按观察组成部分分析\n",
    "print(\"\\n观察组成部分分析:\")\n",
    "print(f\"- 自身状态 (0-10): min={obs[:10].min():.3f}, max={obs[:10].max():.3f}\")\n",
    "print(f\"- 视野内实体 (10-60): min={obs[10:60].min():.3f}, max={obs[10:60].max():.3f}\")\n",
    "print(f\"- 消息信息 (60-70): min={obs[60:70].min():.3f}, max={obs[60:70].max():.3f}\")\n",
    "print(f\"- 其他信息 (70-80): min={obs[70:80].min():.3f}, max={obs[70:80].max():.3f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "智能体角色能力分析:\n",
      "\n",
      "SCOUT:\n",
      "  - 数量: 6\n",
      "  - 视野范围: 5\n",
      "  - 移动速度: 2.0\n",
      "  - 负重能力: 0\n",
      "  - 初始能量: 100.0\n",
      "\n",
      "WORKER:\n",
      "  - 数量: 9\n",
      "  - 视野范围: 3\n",
      "  - 移动速度: 1.0\n",
      "  - 负重能力: 2\n",
      "  - 初始能量: 100.0\n",
      "\n",
      "TRANSPORTER:\n",
      "  - 数量: 3\n",
      "  - 视野范围: 4\n",
      "  - 移动速度: 1.5\n",
      "  - 负重能力: 5\n",
      "  - 初始能量: 100.0\n"
     ]
    }
   ],
   "source": [
    "# 智能体角色能力分析\n",
    "env = create_hrg_env(difficulty=\"normal\")\n",
    "observations = env.reset()\n",
    "\n",
    "print(\"智能体角色能力分析:\")\n",
    "\n",
    "from Env.HRG.core import AgentType\n",
    "\n",
    "role_stats = {\n",
    "    AgentType.SCOUT: [],\n",
    "    AgentType.WORKER: [],\n",
    "    AgentType.TRANSPORTER: []\n",
    "}\n",
    "\n",
    "# 收集几个回合的数据\n",
    "for episode in range(3):\n",
    "    observations = env.reset()\n",
    "    \n",
    "    for agent in env.agents.values():\n",
    "        role_stats[agent.type].append({\n",
    "            'vision_range': agent.config.vision_range,\n",
    "            'move_speed': agent.config.move_speed,\n",
    "            'carry_capacity': agent.config.carry_capacity,\n",
    "            'energy': agent.energy\n",
    "        })\n",
    "\n",
    "# 统计分析\n",
    "for agent_type, stats in role_stats.items():\n",
    "    if stats:\n",
    "        print(f\"\\n{agent_type.name}:\")\n",
    "        print(f\"  - 数量: {len(stats)}\")\n",
    "        print(f\"  - 视野范围: {stats[0]['vision_range']}\")\n",
    "        print(f\"  - 移动速度: {stats[0]['move_speed']}\")\n",
    "        print(f\"  - 负重能力: {stats[0]['carry_capacity']}\")\n",
    "        print(f\"  - 初始能量: {stats[0]['energy']}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "奖励系统分析:\n",
      "回合 1: 总奖励 = -6.900, 得分 = 0.0\n",
      "回合 2: 总奖励 = -8.200, 得分 = 0.0\n",
      "回合 3: 总奖励 = -6.000, 得分 = 0.0\n",
      "回合 4: 总奖励 = -7.500, 得分 = 0.0\n",
      "回合 5: 总奖励 = -6.200, 得分 = 0.0\n",
      "\n",
      "奖励统计分析:\n",
      "\n",
      "scout_0:\n",
      "  - 平均奖励: -0.0284\n",
      "  - 标准差: 0.0387\n",
      "  - 最大奖励: -0.0100\n",
      "  - 最小奖励: -0.1100\n",
      "  - 正奖励比例: 0.00%\n",
      "\n",
      "scout_1:\n",
      "  - 平均奖励: -0.0312\n",
      "  - 标准差: 0.0409\n",
      "  - 最大奖励: -0.0100\n",
      "  - 最小奖励: -0.1100\n",
      "  - 正奖励比例: 0.00%\n",
      "\n",
      "transporter_0:\n",
      "  - 平均奖励: -0.0260\n",
      "  - 标准差: 0.0367\n",
      "  - 最大奖励: -0.0100\n",
      "  - 最小奖励: -0.1100\n",
      "  - 正奖励比例: 0.00%\n",
      "\n",
      "worker_0:\n",
      "  - 平均奖励: -0.0144\n",
      "  - 标准差: 0.0257\n",
      "  - 最大奖励: 0.1900\n",
      "  - 最小奖励: -0.1100\n",
      "  - 正奖励比例: 0.40%\n",
      "\n",
      "worker_1:\n",
      "  - 平均奖励: -0.0204\n",
      "  - 标准差: 0.0305\n",
      "  - 最大奖励: -0.0100\n",
      "  - 最小奖励: -0.1100\n",
      "  - 正奖励比例: 0.00%\n",
      "\n",
      "worker_2:\n",
      "  - 平均奖励: -0.0188\n",
      "  - 标准差: 0.0323\n",
      "  - 最大奖励: 0.1900\n",
      "  - 最小奖励: -0.1100\n",
      "  - 正奖励比例: 0.40%\n",
      "\n",
      "团队平均回合奖励: -6.960 ± 0.816\n"
     ]
    }
   ],
   "source": [
    "# 奖励系统分析\n",
    "env = create_hrg_env(difficulty=\"normal\")\n",
    "\n",
    "print(\"奖励系统分析:\")\n",
    "\n",
    "# 执行多个步骤收集奖励数据\n",
    "all_rewards = {agent_id: [] for agent_id in env.agent_ids}\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(5):\n",
    "    observations = env.reset()\n",
    "    episode_total = 0\n",
    "    \n",
    "    for step in range(50):\n",
    "        actions = {agent_id: np.random.choice(env.get_avail_actions(agent_id)) \n",
    "                  for agent_id in env.agent_ids}\n",
    "        observations, rewards, dones, infos = env.step(actions)\n",
    "        \n",
    "        for agent_id, reward in rewards.items():\n",
    "            all_rewards[agent_id].append(reward)\n",
    "            episode_total += reward\n",
    "        \n",
    "        if any(dones.values()):\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(episode_total)\n",
    "    print(f\"回合 {episode + 1}: 总奖励 = {episode_total:.3f}, 得分 = {env.game_state.total_score:.1f}\")\n",
    "\n",
    "print(\"\\n奖励统计分析:\")\n",
    "for agent_id, rewards in all_rewards.items():\n",
    "    if rewards:\n",
    "        print(f\"\\n{agent_id}:\")\n",
    "        print(f\"  - 平均奖励: {np.mean(rewards):.4f}\")\n",
    "        print(f\"  - 标准差: {np.std(rewards):.4f}\")\n",
    "        print(f\"  - 最大奖励: {np.max(rewards):.4f}\")\n",
    "        print(f\"  - 最小奖励: {np.min(rewards):.4f}\")\n",
    "        print(f\"  - 正奖励比例: {np.mean(np.array(rewards) > 0):.2%}\")\n",
    "\n",
    "print(f\"\\n团队平均回合奖励: {np.mean(episode_rewards):.3f} ± {np.std(episode_rewards):.3f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本教程介绍了HRG环境的主要功能和使用方法：\n",
    "\n",
    "1. **基础环境创建和使用**：展示了如何创建环境、重置、执行步骤\n",
    "2. **CTDE兼容性**：演示了集中式训练分布式执行的支持\n",
    "3. **可视化功能**：展示了静态和动态可视化能力\n",
    "4. **配置管理**：介绍了不同难度和专用配置\n",
    "5. **性能测试**：评估了环境执行效率\n",
    "6. **高级功能**：演示了动作掩码、观察分析等功能\n",
    "7. **完整示例**：提供了简单智能体的实现\n",
    "\n",
    "### 关键特点：\n",
    "- ✅ **异构智能体**：三种不同角色，各具特色\n",
    "- ✅ **部分可观测**：不同视野范围，信息不对称\n",
    "- ✅ **协作需求**：需要智能体团队协作才能高效完成任务\n",
    "- ✅ **CTDE兼容**：支持主流多智能体强化学习算法\n",
    "- ✅ **高性能**：单步执行时间<0.1秒，适合大规模训练\n",
    "- ✅ **可视化**：丰富的可视化功能便于分析和调试\n",
    "- ✅ **可配置**：多种预设和自定义配置支持\n",
    "\n",
    "HRG环境现在可以用于多智能体强化学习的研究和实验了！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
